
@book{abbottTheoreticalNeuroscienceComputational2001,
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  shorttitle = {Theoretical {{Neuroscience}}},
  author = {Abbott, Laurence F. and Dayan, Peter},
  editor = {Sejnowski, Terrence J. and Poggio, Tomaso A.},
  year = {2001},
  month = oct,
  series = {Computational {{Neuroscience Series}}},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {978-0-262-04199-7},
  langid = {english}
}

@article{adriaansCriticalAnalysisFloridi2010,
  title = {A {{Critical Analysis}} of {{Floridi}}'s {{Theory}} of {{Semantic Information}}},
  author = {Adriaans, Pieter},
  year = {2010},
  month = jun,
  journal = {Knowledge, Technology \& Policy},
  volume = {23},
  number = {1},
  pages = {41--56},
  issn = {1874-6314},
  doi = {10.1007/s12130-010-9097-5},
  abstract = {In various publications over the past years, Floridi has developed a theory of semantic information as well-formed, meaningful, and truthful data. This theory is more or less orthogonal to the standard entropy-based notions of information known from physics, information theory, and computer science that all define the amount of information in a certain system as a scalar value without any direct semantic implication. In this context the question rises what the exact relation between these various conceptions of information is and whether there is a real need to enrich these mathematically more or less rigid definitions with a less formal notion of semantic information. I investigate various philosophical aspects of the more formal definitions of information in the light of Floridi's theory. The position I defend is that the formal treatment of the notion of information as a general theory of entropy is one of the fundamental achievements of modern science that in itself is a rich source for new philosophical reflection. This makes information theory a competitor of classical epistemology rather than a servant. In this light Floridi's philosophy of information is more a reprise of classical epistemology that only pays lip service to information theory but fails to address the important central questions of philosophy of information. Specifically, I will defend the view that notions that are associated with truth, knowledge, and meaning all can adequately be reconstructed in the context of modern information theory and that consequently there is no need to introduce a concept of semantic information.},
  langid = {english},
  file = {/Users/tom/Zotero/storage/5752JIWH/Adriaans - 2010 - A Critical Analysis of Floridi’s Theory of Semanti.pdf}
}

@incollection{adriaansInformation2020,
  title = {Information},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Adriaans, Pieter},
  editor = {Zalta, Edward N.},
  year = {2020},
  edition = {Fall 2020},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {Philosophy of Information deals with the philosophical analysis of thenotion of information both from a historical and a systematicperspective. With the emergence of the empiricist theory of knowledgein early modern philosophy, the development of various mathematicaltheories of information in the twentieth century and the rise ofinformation technology, the concept of ``information'' hasconquered a central place in the sciences and in society. Thisinterest also led to the emergence of a separate branch of philosophythat analyzes information in all its guises (Adriaans \& vanBenthem 2008a,b; Lenski 2010; Floridi 2002, 2011). Information hasbecome a central category in both the sciences and the humanities andthe reflection on information influences a broad range ofphilosophical disciplines varying from logic (Dretske 1981; vanBenthem \& van Rooij 2003; van Benthem 2006, see the entry on logic and information), epistemology (Simondon 1989) to ethics (Floridi 1999) and esthetics(Schmidhuber 1997a; Adriaans 2008) to ontology (Zuse 1969; Wheeler1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010). , There is no consensus about the exact nature of the field ofphilosophy of information. Several authors have proposed a more orless coherent philosophy of information as an attempt to rethinkphilosophy from a new perspective: e.g., quantum physics(Mugur-Sch\"achter 2002),  logic (Brenner 2008), semantic information (Floridi 2011; Adams \&de Moraes 2016, see the entry on semantic conceptions of information), communication and message systems (Capurro \&Holgate 2011) and meta-philosophy (Wu 2010, 2016). Others (Adriaans\& van Benthem 2008a; Lenski 2010) see it more as a technicaldiscipline with deep roots in the history of philosophy andconsequences for various disciplines like methodology, epistemologyand ethics. Whatever one's interpretation of the nature ofphilosophy of information is, it seems to imply an ambitious researchprogram consisting of many sub-projects varying from thereinterpretation of the history of philosophy in the context of moderntheories of information, to an in depth analysis of the role ofinformation in science, the humanities and society as a whole. , The term ``information'' in colloquial speech is currentlypredominantly used as an abstract mass-noun used to denote any amountof data, code or text that is stored, sent, received or manipulated inany medium. The detailed history of both the term``information'' and the various concepts that come with itis complex and for the larger part still has to be written (Seiffert1968; Schnelle 1976; Capurro 1978, 2009; Capurro \& Hj\o rland2003). The exact meaning of the term ``information'' variesin different philosophical traditions and its colloquial use variesgeographically and over different pragmatic contexts. Although ananalysis of the notion of information has been a theme in Westernphilosophy from its early inception, the explicit analysis ofinformation as a philosophical concept is recent, and dates back tothe second half of the twentieth century. At this moment it is clearthat information is a pivotal concept in the sciences and humanitiesand in our every day life. Everything we know about the world is basedon information we received or gathered and every science in principledeals with information. There is a network of related concepts ofinformation, with roots in various disciplines like physics,mathematics, logic, biology, economy and epistemology. All thesenotions cluster around two central properties: , The elegance of this formula however does not shield us from theconceptual problems it harbors. In the twentieth century variousproposals for formalization of concepts of information were made: , Until recently the possibility of a unification of these theories wasgenerally doubted (Adriaans \& van Benthem 2008a), but after twodecades of research, perspectives for unification seem better. , The contours of a unified concept of information emerges along thefollowing lines: , The emergence of a coherent theory to measure informationquantitatively in the twentieth century is closely related to thedevelopment of the theory of computing. Central in this context arethe notions of Universality, Turingequivalence and Invariance: because theconcept of a Turing system defines the notion of a universalprogrammable computer, all universal models of computation seem tohave the same power. This implies that all possible measures ofinformation definable for universal models of computation (RecursiveFunctions, Turing Machine, Lambda Calculus etc.) areinvariant modulo an additive constant. This gives a perspective on a unified theory ofinformation that might dominate the research program for the years tocome.},
  file = {/Users/tom/Zotero/storage/82TICS8W/information.html}
}

@article{amitaiRegenerativeActivityApical1993,
  title = {Regenerative Activity in Apical Dendrites of Pyramidal Cells in Neocortex},
  author = {Amitai, Y. and Friedman, A. and Connors, B. W. and Gutnick, M. J.},
  year = {1993 Jan-Feb},
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  volume = {3},
  number = {1},
  pages = {26--38},
  issn = {1047-3211},
  doi = {10.1093/cercor/3.1.26},
  abstract = {In intracellular recordings from three neocortical pyramidal cells in vitro, intracellular dye injection identified the impalement site as the primary trunk of the apical dendrite. Dendritic recordings displayed two types of regenerative events: relatively fast, low-threshold spikes with amplitudes of 12-69 mV, and slower, higher-threshold spikes up to 80 mV in amplitude. This distinctive dendritic firing pattern was also encountered in six recordings without dye-filled electrodes. Fast spike frequency was extremely sensitive to small changes in membrane potential at the recording site. In one recording, the fast spikes were blocked by 1 microM TTX, while slow events were spared. A computational model of a pyramidal cell was constructed to assist in interpreting the recordings. Simulations suggested that the fast spikes were generated primarily by active Na+ conductance concentrated at a distance from the impalement site, probably in the region of the soma. The low threshold of the fast spikes suggested that Na+ channels also exist in the apical dendrites, where they have a relatively low density. The data strongly imply that there are Ca2+ channels in the apical dendrites.},
  langid = {english},
  pmid = {8439739},
  keywords = {Action Potentials,Animals,Cerebral Cortex,Computer Simulation,Dendrites,Guinea Pigs,Nerve Regeneration,Pyramidal Tracts,Rats,Rats; Sprague-Dawley,Synapses}
}

@article{amuntsHumanBrainProject2019,
  title = {The {{Human Brain Project}}\textemdash{{Synergy}} between Neuroscience, Computing, Informatics, and Brain-Inspired Technologies},
  author = {Amunts, Katrin and Knoll, Alois C. and Lippert, Thomas and Pennartz, Cyriel M. A. and Ryvlin, Philippe and Destexhe, Alain and Jirsa, Viktor K. and D'Angelo, Egidio and Bjaalie, Jan G.},
  year = {2019},
  month = jul,
  journal = {PLOS Biology},
  volume = {17},
  number = {7},
  pages = {e3000344},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000344},
  abstract = {The Human Brain Project (HBP) is a European flagship project with a 10-year horizon aiming to understand the human brain and to translate neuroscience knowledge into medicine and technology. To achieve such aims, the HBP explores the multilevel complexity of the brain in space and time; transfers the acquired knowledge to brain-derived applications in health, computing, and technology; and provides shared and open computing tools and data through the HBP European brain research infrastructure. We discuss how the HBP creates a transdisciplinary community of researchers united by the quest to understand the brain, with fascinating perspectives on societal benefits.},
  langid = {english},
  keywords = {Behavioral neuroscience,Cognitive neuroscience,Computational neuroscience,Health informatics,Metadata,Neural networks,Neuroscience,Simulation and modeling},
  file = {/Users/tom/Zotero/storage/S88BN7CM/Amunts et al. - 2019 - The Human Brain Project—Synergy between neuroscien.pdf;/Users/tom/Zotero/storage/Q6H4XSMR/article.html}
}

@misc{andersonPancomputationalismComputationalDescription2017,
  type = {Preprint},
  title = {Pancomputationalism and the {{Computational Description}} of {{Physical Systems}}},
  author = {Anderson, Neal G. and Piccinini, Gualtiero},
  year = {2017},
  month = feb,
  abstract = {According to pancomputationalism, all physical systems \textendash{} atoms, rocks, hurricanes, and toasters \textendash{} perform computations. Pancomputationalism seems to be increasingly popular among some philosophers and physicists. In this paper, we interpret pancomputationalism in terms of computational descriptions of varying strength\textemdash computational interpretations of physical microstates and dynamics that vary in their restrictiveness. We distinguish several types of pancomputationalism and identify essential features of the computational descriptions required to support them. By tying various pancomputationalist theses directly to notions of what counts as computation in a physical system, we clarify the meaning, strength, and plausibility of pancomputationalist claims. We show that the force of these claims is diminished when weaknesses in their supporting computational descriptions are laid bare. Specifically, once computation is meaningfully distinguished from ordinary dynamics, the most sensational pancomputationalist claims are unwarranted, whereas the more modest claims offer little more than recognition of causal similarities between physical processes and the most primitive computing processes.},
  howpublished = {http://philsci-archive.pitt.edu/12812/},
  langid = {english},
  file = {/Users/tom/Zotero/storage/8L24BDY4/Anderson and Piccinini - 2017 - Pancomputationalism and the Computational Descript.pdf;/Users/tom/Zotero/storage/SEQ9Y2CP/12812.html}
}

@book{batesonMendelPrinciplesHeredity2013,
  title = {Mendel's {{Principles}} of {{Heredity}}},
  author = {Bateson, William and Mendel, Gregor},
  year = {2013},
  month = mar,
  publisher = {{Courier Corporation}},
  abstract = {Six years after Charles Darwin announced his theory of evolution to the world, Gregor Mendel began studying the inheritance of traits in pea plants. Mendel\&\#39;s research led to his discovery of dominant and recessive traits and other facts of evolution, which he reported in his groundbreaking 1865 paper, Experiments in Plant Hybridization. His findings languished until 1902, when William Bateson revived interest in the subject with this book, a succinct account of Mendel\&\#39;s heredity-related discoveries. Bateson coined the term \&quot;genetics\&quot; to refer to heredity and inherited traits, and his rediscovery of Mendel\&\#39;s work forms the foundation of today\&\#39;s field of genetics.Suitable for biology and general science students at the undergraduate and graduate levels, this volume is essential reading for anyone with an interest in science and genetics. In addition to Bateson\&\#39;s commentary, it features two of Mendel\&\#39;s papers\textemdash including the original Experiments\textemdash plus a biography of Mendel, a detailed bibliography, and indexes of subjects and authors. Numerous figures complement the text, along with eight pages of color illustrations.},
  googlebooks = {x7ici4OUmkYC},
  isbn = {978-0-486-14837-3},
  langid = {english},
  keywords = {Science / Life Sciences / Biology}
}

@article{bekolayNengoPythonTool2014,
  title = {Nengo: A {{Python}} Tool for Building Large-Scale Functional Brain Models},
  shorttitle = {Nengo},
  author = {Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C. and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron and Eliasmith, Chris},
  year = {2014},
  journal = {Frontiers in Neuroinformatics},
  volume = {7},
  issn = {1662-5196},
  doi = {10.3389/fninf.2013.00048},
  abstract = {Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results.},
  langid = {english},
  keywords = {Control theory,Nengo,neural engineering framework,Neuroscience,python,simulation,theoretical neuroscience},
  file = {/Users/tom/Zotero/storage/DGZ8RCGH/Bekolay et al. - 2014 - Nengo a Python tool for building large-scale func.pdf}
}

@misc{Believers2015,
  title = {The {{Believers}}},
  year = {2015},
  month = feb,
  journal = {The Chronicle of Higher Education},
  abstract = {The hidden story behind the code that runs our lives.},
  chapter = {The Review},
  howpublished = {https://www.chronicle.com/article/the-believers-190147/},
  langid = {english},
  file = {/Users/tom/Zotero/storage/CSCTQY7N/the-believers-190147.html}
}

@article{bialekReadingNeuralCode1991,
  title = {Reading a {{Neural Code}}},
  author = {Bialek, William and Rieke, Fred and van Steveninck, Rob R. de Ruyter and Warland, David},
  year = {1991},
  month = jun,
  journal = {Science},
  publisher = {{American Association for the Advancement of Science}},
  abstract = {Traditional approaches to neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task\textemdash extracting information about an unknown time-dependent stimulus from short segments of a spike ...},
  langid = {english},
  file = {/Users/tom/Zotero/storage/FJ5JSKGS/science.html}
}

@book{bialekSpikesExploringNeural1996,
  title = {Spikes: {{Exploring}} the {{Neural Code}}},
  shorttitle = {Spikes},
  author = {Bialek, William and van Steveninck, Rob de Ruyter and Rieke, Fred and Warland, David},
  editor = {Sejnowski, Terrence J. and Poggio, Tomaso A.},
  year = {1996},
  month = dec,
  series = {Computational {{Neuroscience Series}}},
  publisher = {{A Bradford Book}},
  address = {{Cambridge, MA, USA}},
  abstract = {What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons. Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by "real" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory.},
  isbn = {978-0-262-18174-7},
  langid = {english}
}

@book{brayWetwareComputerEvery2009,
  title = {Wetware: {{A Computer}} in {{Every Living Cell}}},
  shorttitle = {Wetware},
  author = {Bray, Dennis},
  year = {2009},
  month = may,
  publisher = {{Yale University Press}},
  abstract = {``A beautifully written journey into the mechanics of the world of the cell, and even beyond, exploring the analogy with computers in a surprising way'' (Denis Noble, author of Dance to the Tune of Life). ~ How does a single-cell creature, such as an amoeba, lead such a sophisticated life? How does it hunt living prey, respond to lights, sounds, and smells, and display complex sequences of movements without the benefit of a nervous system? This book offers a startling and original answer. ~ In clear, jargon-free language, Dennis Bray taps the findings from the discipline of systems biology to show that the internal chemistry of living cells is a form of computation. Cells are built out of molecular circuits that perform logical operations, as electronic devices do, but with unique properties. Bray argues that the computational juice of cells provides the basis for all distinctive properties of living systems: it allows organisms to embody in their internal structure an image of the world, and this accounts for their adaptability, responsiveness, and intelligence. ~ In Wetware, Bray offers imaginative, wide-ranging, and perceptive critiques of robotics and complexity theory, as well as many entertaining and telling anecdotes. For the general reader, the practicing scientist, and all others with an interest in the nature of life, this book is an exciting portal to some of biology's latest discoveries and ideas. ~ ``Drawing on the similarities between Pac-Man and an amoeba and efforts to model the human brain, this absorbing read shows that biologists and engineers have a lot to learn from working together.'' \textemdash Discover magazine ~ ``Wetware will get the reader thinking.'' \textemdash Science magazine},
  googlebooks = {UL7xW\_FL\_hMC},
  isbn = {978-0-300-15544-0},
  langid = {english},
  keywords = {Science / Life Sciences / Biochemistry,Science / Life Sciences / Cell Biology,Science / Life Sciences / Molecular Biology}
}

@patent{brenesSystemMethodIncreasing2016,
  title = {System and Method for Increasing Data Transmission Rates through a Content Distribution Network with Customized Aggregations},
  author = {Brenes, Jos{\'e} Pablo Gonz{\'a}lez and Goldin, Ilya and Larusson, Johann A. and Behrens, John and McTavish, Thomas},
  year = {2016},
  month = may,
  number = {US20160127244A1},
  assignee = {Pearson Education Inc},
  langid = {english},
  nationality = {US},
  keywords = {aggregation,data,data packet,server,user},
  file = {/Users/tom/Zotero/storage/BZY2GTIP/Brenes et al. - 2016 - System and method for increasing data transmission.pdf}
}

@article{bretteCodingRelevantMetaphor2019,
  title = {Is Coding a Relevant Metaphor for the Brain?},
  author = {Brette, Romain},
  year = {2019/ed},
  journal = {Behavioral and Brain Sciences},
  volume = {42},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X19000049},
  abstract = {``Neural coding'' is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the neural code depends on experimental details that are not carried by the coding variable (e.g., the spike count). Thus, the representational power of neural codes is much more limited than generally implied. Second, neural codes carry information only by reference to things with known meaning. In contrast, perceptual systems must build information from relations between sensory signals and actions, forming an internal model. Neural codes are inadequate for this purpose because they are unstructured and therefore unable to represent relations. Third, coding variables are observables tied to the temporality of experiments, whereas spikes are timed actions that mediate coupling in a distributed dynamical system. The coding metaphor tries to fit the dynamic, circular, and distributed causal structure of the brain into a linear chain of transformations between observables, but the two causal structures are incongruent. I conclude that the neural coding metaphor cannot provide a valid basis for theories of brain function, because it is incompatible with both the causal structure of the brain and the representational requirements of cognition.},
  langid = {english},
  keywords = {action,information,neural coding,perception,sensorimotor},
  file = {/Users/tom/Zotero/storage/Y5L2NSII/Brette - 2019 - Is coding a relevant metaphor for the brain.pdf;/Users/tom/Zotero/storage/BJLT79NR/D578626E4888193FFFAE5B6E2C37E052.html}
}

@article{brettePhilosophySpikeRateBased2015,
  title = {Philosophy of the {{Spike}}: {{Rate-Based}} vs. {{Spike-Based Theories}} of the {{Brain}}},
  shorttitle = {Philosophy of the {{Spike}}},
  author = {Brette, Romain},
  year = {2015},
  journal = {Frontiers in Systems Neuroscience},
  volume = {9},
  pages = {151},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2015.00151},
  abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
  file = {/Users/tom/Zotero/storage/533CLT8N/Brette - 2015 - Philosophy of the Spike Rate-Based vs. Spike-Base.pdf}
}

@article{brunetDamageResponseAction2016,
  title = {From Damage Response to Action Potentials: Early Evolution of Neural and Contractile Modules in Stem Eukaryotes},
  shorttitle = {From Damage Response to Action Potentials},
  author = {Brunet, Thibaut and Arendt, Detlev},
  year = {2016},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {371},
  number = {1685},
  pages = {20150043},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2015.0043},
  abstract = {Eukaryotic cells convert external stimuli into membrane depolarization, which in turn triggers effector responses such as secretion and contraction. Here, we put forward an evolutionary hypothesis for the origin of the depolarization\textendash contraction\textendash secretion (DCS) coupling, the functional core of animal neuromuscular circuits. We propose that DCS coupling evolved in unicellular stem eukaryotes as part of an `emergency response' to calcium influx upon membrane rupture. We detail how this initial response was subsequently modified into an ancient mechanosensory\textendash effector arc, present in the last eukaryotic common ancestor, which enabled contractile amoeboid movement that is widespread in extant eukaryotes. Elaborating on calcium-triggered membrane depolarization, we reason that the first action potentials evolved alongside the membrane of sensory-motile cilia, with the first voltage-sensitive sodium/calcium channels (Nav/Cav) enabling a fast and coordinated response of the entire cilium to mechanosensory stimuli. From the cilium, action potentials then spread across the entire cell, enabling global cellular responses such as concerted contraction in several independent eukaryote lineages. In animals, this process led to the invention of mechanosensory contractile cells. These gave rise to mechanosensory receptor cells, neurons and muscle cells by division of labour and can be regarded as the founder cell type of the nervous system.},
  keywords = {action potentials,electrophysiology,evo-devo,evolution,musculature,nervous systems},
  file = {/Users/tom/Zotero/storage/B7M7UUZM/Brunet and Arendt - 2016 - From damage response to action potentials early e.pdf}
}

@misc{burakbayramliYannLeCunWho2017,
  title = {Yann {{LeCun}} - {{Who Is Afraid}} of {{Non-Convex Functions}}?},
  author = {{Burak Bayramli}},
  year = {2017},
  month = nov,
  abstract = {Neural Nets, training, backprop, convex, non-convex, workshop More content on math and tech https://burakbayramli.github.io/dersb...}
}

@article{capranicaUntuningTuningCurve1992,
  title = {The Untuning of the Tuning Curve: Is It Time?},
  shorttitle = {The Untuning of the Tuning Curve},
  author = {Capranica, Robert R.},
  year = {1992},
  month = dec,
  journal = {Seminars in Neuroscience},
  series = {Communication: {{Behavior}} and {{Neurobiology}}},
  volume = {4},
  number = {6},
  pages = {401--408},
  issn = {1044-5765},
  doi = {10.1016/1044-5765(92)90048-7},
  abstract = {The measures that are currently used to characterize the response properties of auditory neurons, such as tuning curve and Q value, stem primarily from engineering descriptors of electrical circuits. The motivation behind those descriptors in turn derives from the mathematical concepts of Fourier theory and systems (frequency) analysis. It now seems unlikely that the auditory nervous system performs such a frequency analysis and therefore those measures and concept of frequency tuning curves need re-evaluation. It therefore is time that we seriously question the relevance of classical tuning curves for the study of sound communication. The auditory system is uniquely endowed with an ability to process rapid signal variation in time. More attention should be paid to the encoding of temporal waveforms directly in the time domain, rather than invoking a transformation into the frequency domain and the application of mathematical frequency analysis based on engineering principles.},
  langid = {english},
  keywords = {acoustic waveform,auditory system,Fourier analysis,frequency domain,sound communication,temporal domain},
  file = {/Users/tom/Zotero/storage/CEZ68LA9/1044576592900487.html}
}

@book{carrollBigPictureOrigins2017,
  title = {The {{Big Picture}}: {{On}} the {{Origins}} of {{Life}}, {{Meaning}}, and the {{Universe Itself}}},
  shorttitle = {The {{Big Picture}}},
  author = {Carroll, Sean},
  year = {2017},
  month = may,
  edition = {Reprint edition},
  publisher = {{Dutton}},
  isbn = {978-1-101-98425-3},
  langid = {english}
}

@inproceedings{cassidyTrueNorthHighPerformanceLowPower2016,
  title = {{{TrueNorth}} : A {{High-Performance}} , {{Low-Power Neurosynaptic Processor}} for {{Multi-Sensory Perception}} , {{Action}} , and {{Cognition}}},
  shorttitle = {{{TrueNorth}}},
  author = {Cassidy, Andrew S. and Sawada, J. and Merolla, Paul A. and Arthur, John V. and {Alvarez-Icaza}, Rodrigo and Akopyan, Filipp and Jackson, Bryan L. and Modha, Dharmendra S.},
  year = {2016},
  abstract = {IBM's TrueNorth neurosynaptic processor is a radical departure from decades of traditional von Neumann computing. Containing 5.4 billion transistors and fabricated in a 28nm low-power CMOS process technology, TrueNorth contains 1 million neurons and 256 million synapses. With applications ranging from embedded and embodied intelligence to large-scale perceptual analysis of streaming multi-sensory data, this massively parallel processor consumes only 65mW typically.},
  keywords = {CMOS,Cognition,Embedded system,Embedding,Inclusion Body Myositis (disorder),Low-power broadcasting,Numerous,Parallel computing,Synapses,Transistor,TrueNorth},
  file = {/Users/tom/Zotero/storage/QPB4BLHU/Cassidy et al. - 2016 - TrueNorth  a High-Performance , Low-Power Neurosy.pdf}
}

@article{cullheimThreeDimensionalArchitectureDendritic1987,
  title = {Three-{{Dimensional}} Architecture of Dendritic Trees in Type-Identified {$\alpha$}-Motoneurons},
  author = {Cullheim, S. and Fleshman, J. W. and Glenn, L. L. and Burke, R. E.},
  year = {1987},
  journal = {Journal of Comparative Neurology},
  volume = {255},
  number = {1},
  pages = {82--96},
  issn = {1096-9861},
  doi = {10.1002/cne.902550107},
  abstract = {We have studied the spatial distribution of dendrites of type-identified triceps surae {$\alpha$}-motoneurons, labeled intracellularly with HRP, using a variety of analytical approaches that were designed to quantify the ways in which dendrites occupy three-dimensional space. All of the methods indicated a strong tendency for motoneuron dendrites to project radially. However, regions dorsal and ventral to the somata contained fewer dendritic elements, and less membrane area, than expected for complete radial symmetry. Individual dendrites projecting into these regions tended to be smaller than those projecting rostrocaudally or mediolaterally. Nevertheless, the center of mass of membrane area for five of six fully analyzed cells was within 100 {$\mu$}m of the soma and, in all six cells, was located in the same dorsoventral plane as the cell soma. Maps of the projection of dendritic branches onto concentric shells at various radial distances from the soma showed that some regions have high concentrations of branches, sometimes with considerable overlap between branches arising from different stem dendrites, while other regions have relatively few branches, or none at all. Each motoneuron exhibited a different pattern of projection and there were no systematic differences between fast-twitch (type F, including both types FF and FR units) and slow-twitch (type S) motoneurons evident in the patterns of dendritic concentration. Assessment of the three-dimensional territories of individual dendrites showed that dendrites with larger numbers of terminal branches tended to have larger spatial territories. Despite considerable scatter, the results suggest that the density of branches tends to be approximately the same in large and small dendrites, and in F and S cell groups. The results are discussed in relation to the spatial location of synaptic input to motoneurons.},
  langid = {english},
  keywords = {dendrite territory,neuronal symmetry,spatial analysis},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902550107},
  file = {/Users/tom/Zotero/storage/HML3KSZI/cne.html}
}

@article{dalePharmacologyNerveEndings1935,
  title = {Pharmacology and {{Nerve-Endings}}},
  author = {Dale, Henry},
  year = {1935},
  month = jan,
  journal = {Proceedings of the Royal Society of Medicine},
  volume = {28},
  number = {3},
  pages = {319--332},
  publisher = {{SAGE Publications}},
  issn = {0035-9157},
  doi = {10.1177/003591573502800330},
  abstract = {A brief account is given of the scientific career of Walter Ernest Dixon, and of the importance of his work and his influence for the development of Pharmacology in England. It is suggested that the Memorial Lecture may appropriately deal with some matter of new interest, from one of the fields of research in which Dixon himself was active. Special mention is made of his work with Brodie on the physiology and pharmacology of the bronchioles and the pulmonary blood-vessels, as probably showing the beginning of Dixon's interest in the actions of the alkaloids and organic bases which reproduce the effects of autonomic nerves., An account is given of Dixon's early interest in the suggestion, first made by Elliott, that autonomic nerves transmit their effects by releasing, at their endings, specific substances, which reproduce their actions; and of his attempt to obtain experimental support for this conception. After the War it was established by the experiments of O. Loewi; and it is now generally recognized that parasympathetic effects are so transmitted by release of acetylcholine, sympathetic effects by that of a substance related to adrenaline., Very recent evidence indicates that acetylcholine, by virtue of its other (``nicotine-like'') action, also acts as transmitter of activity at synapses in autonomic ganglia, and from motor nerve to voluntary muscle., The terms ``cholinergic'' and ``adrenergic'' have been introduced to describe nerve-fibres which transmit their actions by the release at their endings of acetylcholine, and of a substance related to adrenaline, respectively. It is shown that Langley and Anderson's evidence, long available, as to the kinds of peripheral efferent fibres which can replace one another in regeneration, can be summarized by the statement, that cholinergic can replace cholinergic fibres, and that adrenergic can replace adrenergic fibres; but that fibres of different chemical function cannot replace one another. The bearing of this new evidence on conceptions of the mode of action of ``neuromimetic'' drugs is discussed. The pharmacological problem can now be more clearly defined, and Dixon's participation in further attempts at its solution will be sadly missed.},
  langid = {english},
  file = {/Users/tom/Zotero/storage/CXH3IIYZ/Dale - 1935 - Pharmacology and Nerve-Endings.pdf}
}

@book{dawkinsBlindWatchmakerWhy2015,
  title = {The {{Blind Watchmaker}}: {{Why}} the {{Evidence}} of {{Evolution Reveals}} a {{Universe}} without {{Design}}},
  shorttitle = {The {{Blind Watchmaker}}},
  author = {Dawkins, Richard},
  year = {2015},
  month = sep,
  edition = {Reissue edition},
  publisher = {{W. W. Norton \& Company}},
  address = {{New York}},
  abstract = {Richard Dawkins's classic remains the definitive argument for our modern understanding of evolution.The Blind Watchmaker is the seminal text for understanding evolution today. In the eighteenth century, theologian William Paley developed a famous metaphor for creationism: that of the skilled watchmaker. In The Blind Watchmaker, Richard Dawkins crafts an elegant riposte to show that the complex process of Darwinian natural selection is unconscious and automatic. If natural selection can be said to play the role of a watchmaker in nature, it is a blind one\rule{1em}{1pt}working without foresight or purpose.In an eloquent, uniquely persuasive account of the theory of natural selection, Dawkins illustrates how simple organisms slowly change over time to create a world of enormous complexity, diversity, and beauty. 22 illustrations},
  isbn = {978-0-393-35149-1},
  langid = {english}
}

@book{dawkinsClimbingMountImprobable1996,
  title = {Climbing {{Mount Improbable}}},
  author = {Dawkins, Richard},
  year = {1996},
  month = sep,
  publisher = {{W W Norton \& Co Inc}},
  address = {{New York}},
  isbn = {978-0-393-03930-6},
  langid = {english}
}

@article{dennettRealPatterns1991,
  title = {Real {{Patterns}}},
  author = {Dennett, Daniel C.},
  year = {1991},
  journal = {The Journal of Philosophy},
  volume = {88},
  number = {1},
  pages = {27--51},
  publisher = {{Journal of Philosophy, Inc.}},
  issn = {0022-362X},
  doi = {10.2307/2027085}
}

@inproceedings{duvenaudConvolutionalNetworksGraphs2015,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and {Aspuru-Guzik}, Alan and Adams, Ryan P},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  file = {/Users/tom/Zotero/storage/LBAGQ2I9/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf}
}

@book{eaglemanLivewiredStoryEverChanging2020,
  title = {Livewired: {{The Inside Story}} of the {{Ever-Changing Brain}}},
  shorttitle = {Livewired},
  author = {Eagleman, David},
  year = {2020},
  month = aug,
  publisher = {{Vintage}},
  langid = {english}
}

@book{forterreOriginEvolutionDNA2013,
  title = {Origin and {{Evolution}} of {{DNA}} and {{DNA Replication Machineries}}},
  author = {Forterre, Patrick and Fil{\'e}e, Jonathan and Myllykallio, Hannu},
  year = {2013},
  journal = {Madame Curie Bioscience Database [Internet]},
  publisher = {{Landes Bioscience}},
  abstract = {The transition from the RNA to the DNA world was a major event in the history of life. The invention of DNA required the appearance of enzymatic activities for both synthesis of DNA precursors, retro-transcription of RNA templates and replication of singleand double-stranded DNA molecules. Recent data from comparative genomics, structural biology and traditional biochemistry have revealed that several of these enzymatic activities have been invented independently more than once, indicating that the transition from RNA to DNA genomes was more complex than previously thought. The distribution of the different protein families corresponding to these activities in the three domains of life (Archaea, Eukarya, and Bacteria) is puzzling. In many cases, Archaea and Eukarya contain the same version of these proteins, whereas Bacteria contain another version. However, in other cases, such as thymidylate synthases or type II DNA topoisomerases, the phylogenetic distributions of these proteins do not follow this simple pattern. Several hypotheses have been proposed to explain these observations, including independent invention of DNA and DNA replication proteins, ancient gene transfer and gene loss, and/or nonorthologous replacement. We review all of them here, with more emphasis on recent proposals suggesting that viruses have played a major role in the origin and evolution of the DNA replication proteins and possibly of DNA itself.},
  langid = {english},
  file = {/Users/tom/Zotero/storage/L376CRQB/NBK6360.html}
}

@article{fryeFlyFlightModel2001,
  title = {Fly {{Flight}}: {{A Model}} for the {{Neural Control}} of {{Complex Behavior}}},
  shorttitle = {Fly {{Flight}}},
  author = {Frye, Mark A and Dickinson, Michael H},
  year = {2001},
  month = nov,
  journal = {Neuron},
  volume = {32},
  number = {3},
  pages = {385--388},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(01)00490-1},
  abstract = {Flies exhibit a repertoire of aerial acrobatics unmatched in robustness and aerodynamic sophistication. The exquisite control of this complex behavior emerges from encoding intricate patterns of optic flow, and the translation of these visual signals into the mechanical language of the motor system. Recent advances in experimental design toward more naturalistic visual and mechanosensory stimuli have served to reinforce fly flight as a key model system for understanding how feedback from multiple sensory modalities is integrated to control complex and robust motor behaviors across taxa.},
  file = {/Users/tom/Zotero/storage/NIVYDAFK/Frye and Dickinson - 2001 - Fly Flight A Model for the Neural Control of Comp.pdf;/Users/tom/Zotero/storage/47AGP2PB/S0896627301004901.html}
}

@article{ghadiriSociallyFairKMeans2020,
  title = {Socially {{Fair}} K-{{Means Clustering}}},
  author = {Ghadiri, Mehrdad and Samadi, Samira and Vempala, Santosh},
  year = {2020},
  month = oct,
  journal = {arXiv:2006.10085 [cs, stat]},
  eprint = {2006.10085},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tom/Zotero/storage/DS64WNVT/Ghadiri et al. - 2020 - Socially Fair k-Means Clustering.pdf;/Users/tom/Zotero/storage/RHYXLESQ/2006.html}
}

@misc{godfreyContinuumLogarithmicLinear2016,
  title = {A Continuum among Logarithmic, Linear, and Exponential Functions, and Its Potential to Improve Generalization in Neural Networks},
  author = {Godfrey, Luke B. and Gashler, Michael S.},
  year = {2016},
  month = feb,
  number = {arXiv:1602.01321},
  eprint = {1602.01321},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {We present the soft exponential activation function for artificial neural networks that continuously interpolates between logarithmic, linear, and exponential functions. This activation function is simple, differentiable, and parameterized so that it can be trained as the rest of the network is trained. We hypothesize that soft exponential has the potential to improve neural network learning, as it can exactly calculate many natural operations that typical neural networks can only approximate, including addition, multiplication, inner product, distance, polynomials, and sinusoids.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/tom/Zotero/storage/V6LA6AU6/Godfrey and Gashler - 2016 - A continuum among logarithmic, linear, and exponen.pdf;/Users/tom/Zotero/storage/SCHYQ7E8/1602.html}
}

@article{greerGolgiAnalysesDendritic1987,
  title = {Golgi Analyses of Dendritic Organization among Denervated Olfactory Bulb Granule Cells},
  author = {Greer, Charles A.},
  year = {1987},
  journal = {Journal of Comparative Neurology},
  volume = {257},
  number = {3},
  pages = {442--452},
  issn = {1096-9861},
  doi = {10.1002/cne.902570311},
  abstract = {In the vertebrate olfactory bulb, the primary projection neurons, mitral and tufted cells, have reciprocal dendrodendritic synapses with respective subpopulations of anaxonic interneurons called granule cells. In the neurological murine mutant Purkinje Cell Degeneration (PCD) all mitral cells are lost during early adulthood. As a consequence, a subpopulation of granule cells is deprived of both afferent input and efferent targets. The effect of this event, on the morphology and sublaminar distribution of granule cells was studied with light microscopic Golgi procedures in affected homozygous recessive PCD mutants and normal heterozygous littermate controls. In the control mice, a minimum of three subpopulations were identified predominantly on the basis of the topology of apical dendrites and their spinous processes within the external plexiform layer (EPL) of the olfactory bulb: type I had dentrites extending across the full width of the EPL and a homogeneous distribution of spines; type II had dendritic arbors confined to the deeper EPL; type III had apical dendrites that arborized extensively within the superficial EPL with no arbors or spines present in the deeper EPL. Prior studies suggest that type II cells form connections with mitral cells; type III cells form connections with tufted cells; and type I cells may integrate information from both populations of projection neurons. In the mutant PCD mice, the classification of subpopulations of granule cells proved difficult due to a compression of dendritic arbors within the EPL. Dendritic processes followed a more horizontal tangent relative to the radial orientation seen in control mice. The length of dendritic branches was reduced by approximately 20\% with a corresponding decrease in the number of spines. The density of spines ( \#/1 {\"m}m of dendrite) was constant in both controls and mutants at approximately 0.21. Truncation of the dendrites in the PCD mutants appeared to occur at terminal portions because the number of dendritic bifurcations was equal in both groups of mice. The data are discussed in terms of subpopulations of granule cells in the mouse olfactory bulb, the sublaminar organization of olfactory bulb circuits, and the capacity for survival and plasticity in the reciprocal dendrodendritic circuits mediated by the granule cell spines.},
  langid = {english},
  keywords = {dendrites,plasticity,tufted cells},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902570311},
  file = {/Users/tom/Zotero/storage/6ND5RPLV/cne.html}
}

@article{guoNeuralCodingSpiking2021,
  title = {Neural {{Coding}} in {{Spiking Neural Networks}}: {{A Comparative Study}} for {{Robust Neuromorphic Systems}}},
  shorttitle = {Neural {{Coding}} in {{Spiking Neural Networks}}},
  author = {Guo, Wenzhe and Fouda, Mohammed E. and Eltawil, Ahmed M. and Salama, Khaled Nabil},
  year = {2021},
  journal = {Frontiers in Neuroscience},
  volume = {15},
  pages = {212},
  issn = {1662-453X},
  doi = {10.3389/fnins.2021.638474},
  abstract = {Various hypotheses of information representation in brain, referred to as neural codes, have been proposed to explain the information transmission between neurons. Neural coding plays an essential role in enabling the brain-inspired spiking neural networks (SNNs) to perform different tasks. To search for the best coding scheme, we performed an extensive comparative study on the impact and performance of four important neural coding schemes, namely, rate coding, time-to-first spike (TTFS) coding, phase coding, and burst coding. The comparative study was carried out using a biological 2-layer SNN trained with an unsupervised spike-timing-dependent plasticity (STDP) algorithm. Various aspects of network performance were considered, including classification accuracy, processing latency, synaptic operations (SOPs), hardware implementation, network compression efficacy, input and synaptic noise resilience, and synaptic fault tolerance. The classification tasks on Modified National Institute of Standards and Technology (MNIST) and Fashion-MNIST datasets were applied in our study. For hardware implementation, area and power consumption were estimated for these coding schemes, and the network compression efficacy was analyzed using pruning and quantization techniques. Different types of input noise and noise variations in the datasets were considered and applied. Furthermore, the robustness of each coding scheme to the non-ideality-induced synaptic noise and fault in analog neuromorphic systems was studied and compared. Our results show that TTFS coding is the best choice in achieving the highest computational performance with very low hardware implementation overhead. TTFS coding requires 4x/7.5x lower processing latency and 3.5x/6.5x fewer SOPs than rate coding during the training/inference process. Phase coding is the most resilient scheme to input noise. Burst coding offers the highest network compression efficacy and the best overall robustness to hardware non-idealities for both training and inference processes. The study presented in this paper reveals the design space created by the choice of each coding scheme, allowing designers to frame each scheme in terms of its strength and weakness given a designs' constraints and considerations in neuromorphic systems.},
  file = {/Users/tom/Zotero/storage/QZRGGJ63/Guo et al. - 2021 - Neural Coding in Spiking Neural Networks A Compar.pdf}
}

@book{HandbookofBrainMicrocircuits,
  title = {Handbook of Brain Microcircuits},
  author = {Gordon Shepherd, MD, DPhil and Sten Grillner, MD},
  year = {2012},
  month = sep,
  publisher = {{Oxford University Press}},
  address = {{Oxford, UK}},
  doi = {10.1093/med/9780195389883.001.0001},
  isbn = {978-0-19-996513-7}
}

@article{harrisMorphologyPhysiologicallyIdentified1986,
  title = {Morphology of Physiologically Identified Thalamocortical Relay Neurons in the Rat Ventrobasal Thalamus},
  author = {Harris, Roger M.},
  year = {1986},
  journal = {Journal of Comparative Neurology},
  volume = {251},
  number = {4},
  pages = {491--505},
  issn = {1096-9861},
  doi = {10.1002/cne.902510405},
  abstract = {The anatomical structure of physiologically identified neurons of the rat ventrobasal thalamus was studied in order to determine if there are morphologically distinct subsets of neurons that correlate with the somatosensory submodalities processed by these cells. Intracellular recordings were used to determine the modality and receptive field of a neuron, after which horseradish peroxidase was iontophoretically injected into the cell, allowing it to be histologically visualized. Computer-assisted measurements of the labeled cells were made to quantitatively analyze the dendritic structure. Cells were divided into physiological groups stimulated by whiskers, glabrous skin, furry skin, noxious stimulation, or joint rotation. Qualitatively, all cells appeared similar, with the same types of branching patterns. Dendritic spines and long, sinuous appendages were found on all distal dendrites. Quantitatively, no statistically significant differences in dendritic structure were found between functionally defined groups with the aid of a number of parameters, including a fitted dendritic ellipse. There was a weak correlation between somal cross-sectional area and receptive field size, suggesting larger cells processed larger receptive fields. In summary, the ventrobasal thalamus of the rat, in contrast to that of higher mammals, appears to contain only one major cell type and to have a very simple neuronal circuitry.},
  langid = {english},
  keywords = {computer-assisted morphometry,dendrite,HRP,intracellular,light microscopy},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902510405},
  file = {/Users/tom/Zotero/storage/TKV3ZIZR/cne.html}
}

@article{hausserHodgkinHuxleyTheoryAction2000,
  title = {The {{Hodgkin-Huxley}} Theory of the Action Potential},
  author = {H{\"a}usser, Michael},
  year = {2000},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {3},
  number = {11},
  pages = {1165--1165},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/81426},
  copyright = {2000 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/tom/Zotero/storage/SY364UTM/Häusser - 2000 - The Hodgkin-Huxley theory of the action potential.pdf;/Users/tom/Zotero/storage/MT2N6AJV/nn1100_1165.html}
}

@misc{haydenWhatDarwinDidn2009,
  title = {What {{Darwin Didn}}'t {{Know}}},
  author = {Hayden, Thomas},
  year = {2009},
  month = feb,
  journal = {Smithsonian Magazine},
  abstract = {Today's scientists marvel that the 19th-century naturalist's grand vision of evolution is still the key to life},
  chapter = {Science, Wildlife, , Articles},
  howpublished = {https://www.smithsonianmag.com/science-nature/what-darwin-didnt-know-45637001/},
  langid = {english},
  file = {/Users/tom/Zotero/storage/TFZK58DJ/what-darwin-didnt-know-45637001.html}
}

@article{heavenWhyDeeplearningAIs2019,
  title = {Why Deep-Learning {{AIs}} Are so Easy to Fool},
  author = {Heaven, Douglas},
  year = {2019},
  month = oct,
  journal = {Nature},
  volume = {574},
  number = {7777},
  pages = {163--166},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-03013-5},
  abstract = {Artificial-intelligence researchers are trying to fix the flaws of neural networks.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Computer science,Information technology},
  annotation = {Bandiera\_abtest: a Cg\_type: News Feature Subject\_term: Computer science, Information technology},
  file = {/Users/tom/Zotero/storage/7KLF4TCR/Heaven - 2019 - Why deep-learning AIs are so easy to fool.pdf;/Users/tom/Zotero/storage/SMSH568T/d41586-019-03013-5.html}
}

@article{henryOrientationSpecificityCells1974,
  title = {Orientation Specificity of Cells in Cat Striate Cortex.},
  author = {Henry, G H and Dreher, B and Bishop, P O},
  year = {1974},
  month = nov,
  journal = {Journal of Neurophysiology},
  volume = {37},
  number = {6},
  pages = {1394--1409},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.1974.37.6.1394}
}

@article{hintonFastLearningAlgorithm2006a,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  journal = {Neural Computation},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use ``complementary priors'' to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  file = {/Users/tom/Zotero/storage/TZLDG4VB/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets.html}
}

@misc{hintonNeuralNetworksMachine2012,
  title = {Neural {{Networks}} for {{Machine Learning}}},
  author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year = {2012},
  langid = {english},
  file = {/Users/tom/Zotero/storage/6AVIFIV7/Hinton et al. - Neural Networks for Machine Learning.pdf}
}

@article{hodgkinQuantitativeDescriptionMembrane1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = {1952},
  month = aug,
  journal = {The Journal of Physiology},
  volume = {117},
  number = {4},
  pages = {500--544},
  issn = {0022-3751},
  pmcid = {PMC1392413},
  pmid = {12991237}
}

@article{hubelReceptiveFieldsFunctional1968,
  title = {Receptive Fields and Functional Architecture of Monkey Striate Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1968},
  journal = {The Journal of Physiology},
  volume = {195},
  number = {1},
  pages = {215--243},
  issn = {1469-7793},
  doi = {10.1113/jphysiol.1968.sp008455},
  abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded. 2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent. 3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other. 4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven. 5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
  langid = {english},
  annotation = {\_eprint: https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1968.sp008455},
  file = {/Users/tom/Zotero/storage/N45C34A3/Hubel and Wiesel - 1968 - Receptive fields and functional architecture of mo.pdf;/Users/tom/Zotero/storage/7CUSTVXN/jphysiol.1968.html}
}

@article{hubelReceptiveFieldsSingle1959,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1959},
  month = oct,
  journal = {The Journal of Physiology},
  volume = {148},
  number = {3},
  pages = {574--591},
  issn = {0022-3751},
  pmcid = {PMC1363130},
  pmid = {14403679},
  file = {/Users/tom/Zotero/storage/S7GQUZWB/Hubel and Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf}
}

@article{huhGradientDescentSpiking2017,
  title = {Gradient {{Descent}} for {{Spiking Neural Networks}}},
  author = {Huh, Dongsung and Sejnowski, Terrence J.},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.04698 [cs, q-bio, stat]},
  eprint = {1706.04698},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (\textasciitilde millisecond) spike-based interactions for efficient encoding of information, and a delayed memory XOR task over extended duration (\textasciitilde second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/tom/Zotero/storage/PTY5H7GM/Huh and Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf;/Users/tom/Zotero/storage/C8EKM6TF/1706.html}
}

@book{humphriesSpikeEpicJourney2021,
  title = {The {{Spike}}: {{An Epic Journey Through}} the {{Brain}} in 2.1 {{Seconds}}},
  shorttitle = {The {{Spike}}},
  author = {Humphries, Mark},
  year = {2021},
  month = mar,
  publisher = {{Princeton University Press}},
  abstract = {The story of a neural impulse and what it reveals about how our brains workWe see the last cookie in the box and think, can I take that? We reach a hand out. In the 2.1 seconds that this impulse travels through our brain, billions of neurons communicate with one another, sending blips of voltage through our sensory and motor regions. Neuroscientists call these blips ``spikes.'' Spikes enable us to do everything: talk, eat, run, see, plan, and decide. In The Spike, Mark Humphries takes readers on the epic journey of a spike through a single, brief reaction. In vivid language, Humphries tells the story of what happens in our brain, what we know about spikes, and what we still have left to understand about them.Drawing on decades of research in neuroscience, Humphries explores how spikes are born, how they are transmitted, and how they lead us to action. He dives into previously unanswered mysteries: Why are most neurons silent? What causes neurons to fire spikes spontaneously, without input from other neurons or the outside world? Why do most spikes fail to reach any destination? Humphries presents a new vision of the brain, one where fundamental computations are carried out by spontaneous spikes that predict what will happen in the world, helping us to perceive, decide, and react quickly enough for our survival.Traversing neuroscience's expansive terrain, The Spike follows a single electrical response to illuminate how our extraordinary brains work.},
  langid = {english}
}

@article{johnSemanticGraphsMathematics2015,
  title = {Semantic Graphs for Mathematics Word Problems Based on Mathematics Terminology},
  author = {John, Rogers Jeffrey Leo and McTavish, Thomas S. and Passonneau, Rebecca J.},
  year = {2015},
  journal = {Graph-Based Educational Data Mining (G-EDM)}
}

@inproceedings{johnSemanticSimilarityGraphs2015,
  title = {Semantic {{Similarity Graphs}} of {{Mathematics Word Problems}}: {{Can Terminology Detection Help}}?},
  shorttitle = {Semantic {{Similarity Graphs}} of {{Mathematics Word Problems}}},
  booktitle = {Proceedings of the {{Eighth International Conference}} on {{Educational Data Mining}}},
  author = {John, Rogers Jeffrey Leo and Passonneau, Rebecca J. and McTavish, Thomas S.},
  year = {2015},
  address = {{Madrid, Spain}}
}

@patent{jrContentDatabaseGeneration2017,
  title = {Content Database Generation},
  author = {Jr, Jose GONZALES-BRENES and Goldin, Ilya and Larusson, Johann A. and Behrens, John and Mctavish, Thomas and Rho, Yun Jin and Anderson, Jacob M. and Kukartsev, Gennadiy A.},
  year = {2017},
  month = oct,
  number = {US20170308556A1},
  assignee = {Pearson Education Inc},
  langid = {english},
  nationality = {US},
  keywords = {aggregation,data,embodiments,several,user},
  file = {/Users/tom/Zotero/storage/ARDPEU2U/Jr et al. - 2017 - Content database generation.pdf}
}

@article{kashiwadaniSynchronizedOscillatoryDischarges1999,
  title = {Synchronized {{Oscillatory Discharges}} of {{Mitral}}/{{Tufted Cells With Different Molecular Receptive Ranges}} in the {{Rabbit Olfactory Bulb}}},
  author = {Kashiwadani, Hideki and Sasaki, Yasnory F. and Uchida, Naoshige and Mori, Kensaku},
  year = {1999},
  month = oct,
  journal = {Journal of Neurophysiology},
  volume = {82},
  number = {4},
  pages = {1786--1792},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.1999.82.4.1786},
  abstract = {Individual glomeruli in the mammalian olfactory bulb represent a single or a few type(s) of odorant receptors. Signals from different types of receptors are thus sorted out into different glomeruli. How does the neuronal circuit in the olfactory bulb contribute to the combination and integration of signals received by different glomeruli? Here we examined electrophysiologically whether there were functional interactions between mitral/tufted cells associated with different glomeruli in the rabbit olfactory bulb. First, we made simultaneous recordings of extracellular single-unit spike responses of mitral/tufted cells and oscillatory local field potentials in the dorsomedial fatty acid\textendash responsive region of the olfactory bulb in urethan-anesthetized rabbits. Using periodic artificial inhalation, the olfactory epithelium was stimulated with a homologous series ofn-fatty acids or n-aliphatic aldehydes. The odor-evoked spike discharges of mitral/tufted cells tended to phase-lock to the oscillatory local field potential, suggesting that spike discharges of many cells occur synchronously during odor stimulation. We then made simultaneous recordings of spike discharges from pairs of mitral/tufted cells located 300\textendash 500 {$\mu$}m apart and performed a cross-correlation analysis of their spike responses to odor stimulation. In {$\sim$}27\% of cell pairs examined, two cells with distinct molecular receptive ranges showed synchronized oscillatory discharges when olfactory epithelium was stimulated with one or a mixture of odorant(s) effective in activating both. The results suggest that the neuronal circuit in the olfactory bulb causes synchronized spike discharges of specific pairs of mitral/tufted cells associated with different glomeruli and the synchronization of odor-evoked spike discharges may contribute to the temporal binding of signals derived from different types of odorant receptor.},
  file = {/Users/tom/Zotero/storage/453AUII7/Kashiwadani et al. - 1999 - Synchronized Oscillatory Discharges of MitralTuft.pdf}
}

@article{kimCentersurroundVsDistanceindependent2012,
  title = {Center-Surround vs. Distance-Independent Lateral Connectivity in the Olfactory Bulb.},
  author = {Kim, David H. and Chang, Andrew Y. and McTavish, Thomas S. and Patel, Hetal K. and Willhite, David C.},
  year = {2012},
  journal = {Front Neural Circuits},
  volume = {6},
  pages = {34--34}
}

@inproceedings{kleindessnerFairKCenterClustering2019,
  title = {Fair K-{{Center Clustering}} for {{Data Summarization}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kleindessner, Matth{\"a}us and Awasthi, Pranjal and Morgenstern, Jamie},
  year = {2019},
  month = may,
  pages = {3448--3457},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In data summarization we want to choose \$k\$ prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose \$k\_i\$ prototypes belonging to group \$i\$. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as \$k\$-center. A natural extension then is to incorporate the fairness constraint into the clustering problem. Existing algorithms for doing so run in time super-quadratic in the size of the data set, which is in contrast to the standard \$k\$-center problem being approximable in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the \$k\$-center problem under the fairness constraint with running time linear in the size of the data set and \$k\$. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead.},
  langid = {english},
  file = {/Users/tom/Zotero/storage/UJPSMQES/Kleindessner et al. - 2019 - Fair k-Center Clustering for Data Summarization.pdf;/Users/tom/Zotero/storage/X8VGUX5M/Kleindessner et al. - 2019 - Fair k-Center Clustering for Data Summarization.pdf}
}

@techreport{laiSkillsTodayWhat2017,
  title = {Skills for {{Today}}: {{What We Know}} about {{Teaching}} and {{Assessing Collaboration}}},
  shorttitle = {Skills for {{Today}}},
  author = {Lai, Emily and DiCerbo, Kristen and Foltz, Peter},
  year = {2017},
  institution = {{Pearson}},
  abstract = {Collaboration is increasingly identified as an important educational outcome, and most models of twenty-first-century skills include collaboration as a key skill (e.g., Griffin, McGaw, \& Care, 2012; Pellegrino \& Hilton, 2012; OECD PISA Collaborative Problem Solving Expert Working Group, 2013; Trilling \& Fadel, 2009). Such widespread emphasis on collaboration skills can be traced to several factors: (1) research suggests that people with good collaboration skills enjoy better performance in school; and (2) research suggests that those with more developed collaboration skills earn recognition on the job from their managers and peers. The P21 Framework for 21st Century Learning (www.P21.org/Framework) includes collaboration as one of its four key concepts (the Four Cs), along with creativity, critical thinking, and communication. This paper is the first in a series to be jointly released by Pearson and P21 entitled, "Skills for Today." Each paper summarizes what is currently known about teaching and assessing one of the Four Cs: collaboration, critical thinking, creativity, and communication. [Sponsors are acknowledged on p31 of the document.]},
  langid = {english},
  keywords = {Communication Skills,Cooperation,Cooperative Learning,Creativity,Critical Thinking,Definitions,Elementary Secondary Education,Evaluation Methods,Feedback (Response),Group Activities,Higher Education,Learning Activities,Models,Scoring,Skill Development,Teaching Methods},
  file = {/Users/tom/Zotero/storage/W9GKG36G/eric.ed.gov.html}
}

@article{laughlinCommunicationNeuronalNetworks2003,
  title = {Communication in {{Neuronal Networks}}},
  author = {Laughlin, Simon B. and Sejnowski, Terrence J.},
  year = {2003},
  month = sep,
  journal = {Science (New York, N.Y.)},
  volume = {301},
  number = {5641},
  pages = {1870--1874},
  issn = {0036-8075},
  doi = {10.1126/science.1089662},
  abstract = {Brains perform with remarkable efficiency, are capable of prodigious computation, and are marvels of communication. We are beginning to understand some of the geometric, biophysical, and energy constraints that have governed the evolution of cortical networks. To operate efficiently within these constraints, nature has optimized the structure and function of cortical networks with design principles similar to those used in electronic networks. The brain also exploits the adaptability of biological systems to reconfigure in response to changing needs.},
  pmcid = {PMC2930149},
  pmid = {14512617},
  file = {/Users/tom/Zotero/storage/4TXY7IFY/Laughlin and Sejnowski - 2003 - Communication in Neuronal Networks.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989a,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  file = {/Users/tom/Zotero/storage/9X5UT7QV/Backpropagation-Applied-to-Handwritten-Zip-Code.html}
}

@misc{lecunWhoAfraidNon2007,
  title = {Who Is Afraid of Non\-convex Loss Functions?},
  author = {LeCun, Yann},
  year = {2007},
  month = dec,
  howpublished = {https://cs.nyu.edu/\textasciitilde yann/talks/lecun-20071207-nonconvex.pdf},
  langid = {english},
  file = {/Users/tom/Zotero/storage/5QDGYWGU/LeCun - Who is afraid of non­convex loss functions.pdf}
}

@article{liebeskindEvolutionSodiumChannels2011,
  title = {Evolution of Sodium Channels Predates the Origin of Nervous Systems in Animals},
  author = {Liebeskind, B. J. and Hillis, D. M. and Zakon, H. H.},
  year = {2011},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {22},
  pages = {9154--9159},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1106363108},
  langid = {english},
  file = {/Users/tom/Zotero/storage/GP3HWIE7/Liebeskind et al. - 2011 - Evolution of sodium channels predates the origin o.pdf}
}

@article{linProgrammingSpikingNeural2018,
  title = {Programming {{Spiking Neural Networks}} on {{Intel}}'s {{Loihi}}},
  author = {Lin, C. and Wild, A. and Chinya, G. N. and Cao, Y. and Davies, M. and Lavery, D. M. and Wang, H.},
  year = {2018},
  month = mar,
  journal = {Computer},
  volume = {51},
  number = {3},
  pages = {52--61},
  issn = {0018-9162},
  doi = {10.1109/MC.2018.157113521},
  abstract = {Loihi is Intel's novel, manycore neuromorphic processor and is the first of its kind to feature a microcode-programmable learning engine that enables on-chip training of spiking neural networks (SNNs). The authors present the Loihi toolchain, which consists of an intuitive Python-based API for specifying SNNs, a compiler and runtime for building and executing SNNs on Loihi, and several target platforms (Loihi silicon, FPGA, and functional simulator). To showcase the toolchain, the authors describe how to build, train, and use a SNN to classify handwritten digits from the MNIST database.},
  keywords = {application program interfaces,compiler,Computational modeling,field programmable gate arrays,FPGA,functional simulator,handwritten character recognition,handwritten digits,Intel Loihi silicon,intuitive Python-based API,learning (artificial intelligence),Loihi toolchain,manycore neuromorphic processor,Mathematical model,microcode-programmable learning engine,MNIST database,neural chips,neural networks,neuromorphic computing,Neuromorphic engineering,neuromorphic processor,on-chip training,Programming,programming paradigms,SNN,spiking neural networks,Synapses},
  file = {/Users/tom/Zotero/storage/E5N3GFTX/8303802.html}
}

@article{lopez-munozNeuronTheoryCornerstone2006,
  title = {Neuron Theory, the Cornerstone of Neuroscience, on the Centenary of the {{Nobel Prize}} Award to {{Santiago Ram\'on}} y {{Cajal}}},
  author = {{L{\'o}pez-Mu{\~n}oz}, Francisco and Boya, Jes{\'u}s and Alamo, Cecilio},
  year = {2006},
  month = oct,
  journal = {Brain Research Bulletin},
  volume = {70},
  number = {4},
  pages = {391--405},
  issn = {0361-9230},
  doi = {10.1016/j.brainresbull.2006.07.010},
  abstract = {Exactly 100 years ago, the Nobel Prize for Physiology and Medicine was awarded to Santiago Ram\'on y Cajal, ``in recognition of his meritorious work on the structure of the nervous system''. Cajal's great contribution to the history of science is undoubtedly the postulate of neuron theory. The present work makes a historical analysis of the circumstances in which Cajal formulated his theory, considering the authors and works that influenced his postulate, the difficulties he encountered for its dissemination, and the way it finally became established. At the time when Cajal began his neurohistological studies, in 1887, Gerlach's reticular theory (a diffuse protoplasmic network of the grey matter of the nerve centres), also defended by Golgi, prevailed among the scientific community. In the first issue of the Revista Trimestral de Histolog\'ia Normal y Patol\'ogica (May, 1888), Cajal presented the definitive evidence underpinning neuron theory, thanks to staining of the axon of the small, star-shaped cells of the molecular layer of the cerebellum of birds, whose collaterals end up surrounding the Purkinje cell bodies, in the form of baskets or nests. He thus demonstrated once and for all that the relationship between nerve cells was not one of continuity, but rather of contiguity. Neuron theory is one of the principal scientific conquests of the 20th century, and which has withstood, with scarcely any modifications, the passage of more than a 100 years, being reaffirmed by new technologies, as the electron microscopy. Today, no neuroscientific discipline could be understood without recourse to the concept of neuronal individuality and nervous transmission at a synaptic level, as basic units of the nervous system.},
  keywords = {Cajal,History of neuroscience,Neuron theory,Reticular theory},
  file = {/Users/tom/Zotero/storage/H9TPF5J3/S0361923006002334.html}
}

@article{maslimStagesStructuralDifferentiation1986,
  title = {Stages in the Structural Differentiation of Retinal Ganglion Cells},
  author = {Maslim, Juliani and Webster, Maree and Stone, Jonathan},
  year = {1986},
  journal = {Journal of Comparative Neurology},
  volume = {254},
  number = {3},
  pages = {382--402},
  issn = {1096-9861},
  doi = {10.1002/cne.902540310},
  abstract = {Using a cultured wholemount technique we have studied the morphological differentiation of ganglion cells in the retina of the rat and cat, during normal development. In both species the differentiation of ganglion cells begins in embryonic life, before embryonic day (E) 17 in the rat and E36 in the cat. It is useful to describe the morphological differentiation of ganglion cells as occurring in three stages. In the first stage, each germinal cell becoming a ganglion cell extends an axon into the fibre layer of the retina and towards the optic disc, and the soma of the cell moves towards the ganglion cell layer. As the soma approaches the ganglion cell layer, the processes that attach its poles to the inner and outer surfaces of the retina are withdrawn. When the soma reaches the ganglion cell layer, a stage of active dendritic growth begins, which lasts until shortly before birth in the cat and until several days after birth in the rat. The cell extends stem dendrites that branch profusely and are commonly tipped by growth cones. The major morphological classes of ganglion cell become distinct in the latter part of stage 2, as do the centroperipheral gradients in ganglion cell size apparent in the cat. During the third stage, the dendritic trees of ganglion cells no longer branch or extend by means of active growth cones. Very considerable growth of all parameters of the cell (soma size, dendrite calibre and length, axon calibre) occurs nevertheless, presumably by interstitial addition of membrane throughout the cell.},
  langid = {english},
  keywords = {development,differentiation,morphology,retina},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902540310},
  file = {/Users/tom/Zotero/storage/BDK4QQF4/cne.html}
}

@inproceedings{mctavishCharacterizationInhibitoryGates2007,
  title = {Characterization of Inhibitory Gates in Mitral Cell Pairs.},
  booktitle = {Association for {{Chemoreception Sciences}}},
  author = {McTavish, Thomas S. and Hunter, Lawrence E. and Schoppa, Nathan E. and Restrepo, Diego},
  year = {2007},
  month = apr,
  address = {{Sarasota, FL}}
}

@inproceedings{mctavishComputationallyFasterMitral2008,
  title = {A {{Computationally Faster Mitral Cell Model}}},
  booktitle = {{{CHEMICAL SENSES}}},
  author = {McTavish, Thomas S. and Restrepo, Diego},
  year = {2008},
  volume = {33},
  pages = {S65--S66},
  publisher = {{OXFORD UNIV PRESS GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND}}
}

@inproceedings{mctavishDiscoveringDescribingTypes2014,
  title = {Discovering and {{Describing Types}} of {{Mathematical Errors}}},
  booktitle = {Proceedings of the {{Seventh International Conference}} on {{Educational Data Mining}}},
  author = {McTavish, Thomas S. and Larusson, Johann Ari},
  year = {2014},
  address = {{London, England}}
}

@inproceedings{mctavishErrorFeatureExtractions2014,
  title = {Error {{Feature Extractions}} from {{Algebra Templates}}},
  booktitle = {Proceedings of the 76th {{Annual Meeting}} of the {{National Council}} on {{Measurement}} in {{Education}} ({{NCME}})},
  author = {McTavish, Thomas S. and Larusson, Johann A.},
  year = {2014},
  month = apr,
  address = {{Philadelphia, PA}}
}

@incollection{mctavishEvolvingSolutionsGenetic2008,
  title = {Evolving Solutions: {{The}} Genetic Algorithm and Evolution Strategies for Finding Optimal Parameters},
  shorttitle = {Evolving Solutions},
  booktitle = {Applications of {{Computational Intelligence}} in {{Biology}}},
  author = {McTavish, Thomas and Restrepo, Diego},
  year = {2008},
  pages = {55--78},
  publisher = {{Springer}}
}

@inproceedings{mctavishFacilitatingGraphInterpretation2014,
  title = {Facilitating {{Graph Interpretation}} via {{Interactive Hierarchical Edges}}.},
  booktitle = {Graph-{{Based Educational Data Mining}} ({{G-EDM}})},
  author = {McTavish, Thomas},
  year = {2014},
  address = {{London, England}}
}

@inproceedings{mctavishGapJunctionCoupling2009,
  title = {Gap Junction Coupling and Granule Cell Connectivity Both Contribute to Long-Range Synchrony in the Olfactory Bulb.},
  booktitle = {Association for {{Chemoreception Sciences}}},
  author = {McTavish, Thomas S. and Restrepo, Diego and Schoppa, Nathan E},
  year = {2009},
  month = apr,
  address = {{Sarasota, FL}}
}

@article{mctavishGatingEffectsMitral2007,
  title = {Gating Effects along Mitral Cell Lateral Dendrites},
  author = {McTavish, Thomas and Hunter, Larry and Schoppa, Nathan and Restrepo, Diego},
  year = {2007},
  journal = {BMC Neuroscience},
  volume = {8},
  number = {2},
  pages = {1}
}

@incollection{mctavishLabelingMathematicalErrors2014,
  title = {Labeling {{Mathematical Errors}} to {{Reveal Cognitive States}}},
  booktitle = {Open {{Learning}} and {{Teaching}} in {{Educational Communities}}},
  author = {McTavish, Thomas S. and Larusson, Johann Ari},
  year = {2014},
  pages = {446--451},
  publisher = {{Springer}},
  address = {{Graz, Austria}}
}

@inproceedings{mctavishLeveragingStudentInteractivity2014,
  title = {Leveraging Student Interactivity Data to Inform Instruction and Monitor Learning Progress},
  booktitle = {Proceedings of the 2014 {{Annual Meeting}} of the {{American Educational Research Association}} ({{AERA}}).},
  author = {McTavish, Thomas S. and Larusson, Johann A.},
  year = {2014},
  month = apr,
  address = {{Philadelphia, PA}}
}

@inproceedings{mctavishMinimalOscillatoryCurrents2006,
  title = {Minimal {{Oscillatory Currents}} in {{Neurons}}.},
  booktitle = {International {{Society}} for {{Computational Biology}}, {{Rocky Mountain Regional Meeting}}},
  author = {McTavish, Thomas S.},
  year = {2006},
  month = dec,
  address = {{Snowmass, CO}}
}

@article{mctavishMitralCellSpike2012,
  title = {Mitral Cell Spike Synchrony Modulated by Dendrodendritic Synapse Location},
  author = {McTavish, Thomas S. and Migliore, Michele and Shepherd, Gordon M. and Hines, Michael L.},
  year = {2012},
  journal = {Frontiers in computational neuroscience},
  volume = {6},
  pages = {3}
}

@phdthesis{mctavishModelingLongrangeSynchrony2009,
  title = {Modeling Long-Range Synchrony in the Olfactory Bulb},
  author = {McTavish, Thomas Scott},
  year = {2009},
  school = {UNIVERSITY OF COLORADO HEALTH SCIENCES CENTER}
}

@inproceedings{mctavishModulationMitralCell2011,
  title = {Modulation of Mitral Cell Spikes by Dendrodendritic Synapse Location.},
  booktitle = {Society for {{Neuroscience}}},
  author = {McTavish, Thomas S. and Migliore, Michele and Hines, Michael L and Shepherd, Gordon M},
  year = {2011},
  month = nov,
  address = {{Washington, DC}}
}

@article{mctavishMulticompartmentLeakyIntegrate2009,
  title = {Multicompartment Leaky Integrate and Fire Neuron Modeling with Multiexponentials},
  author = {McTavish, Thomas S. and Hunter, Lawrence E. and Restrepo, Diego},
  year = {2009},
  journal = {BMC Neuroscience},
  volume = {10},
  number = {Suppl 1},
  pages = {P277}
}

@article{mctavishNeuronBiophysicsIntro,
  title = {Neuron {{Biophysics Intro}}: 2-{{Cell}} Membrane as an Electrical Circuit},
  shorttitle = {Neuron {{Biophysics Intro}}},
  author = {McTavish, Thomas}
}

@inproceedings{mctavishNEURONSimulationsModel2010,
  title = {{{NEURON}} Simulations and Model Viewing over the Web.},
  booktitle = {Society for {{Neuroscience}}},
  author = {McTavish, Thomas S. and Morse, Thomas M. and Hines, Michael L. and Shepherd, Gordon M.},
  year = {2010},
  month = nov,
  address = {{San Diego, CA}}
}

@inproceedings{mctavishSageNEURONTheoretical2010,
  title = {Sage and {{NEURON}}: {{Theoretical}} Neuroscience Education and Analysis over the Web.},
  booktitle = {Society for {{Neuroscience}}},
  author = {McTavish, Thomas S. and Morse, Thomas M. and Hines, Michael L. and Shepherd, Gordon M.},
  year = {2010},
  month = nov,
  address = {{San Diego, CA}}
}

@inproceedings{mctavishSegmentationApproachesVisible2002,
  title = {Segmentation Approaches in the {{Visible Human}}.},
  booktitle = {Colorado {{Alliance}} for {{Bioengineering Annual Meeting}}},
  author = {McTavish, Thomas S.},
  year = {2002},
  address = {{Aurora, CO}}
}

@inproceedings{mctavishTemporalModulationOlfactory2011,
  title = {Temporal Modulation of Olfactory Signals from Dendrodendritic Synaptic Clusters},
  booktitle = {National {{Institutes}} of {{Health National Library}} of {{Medicine Informatics Training Conference}}},
  author = {McTavish, Thomas S. and Migliore, Michele and Hines, Michael L. and Shepherd, Gordon M. and Miller, Perry},
  year = {2011},
  month = jul,
  address = {{Bethesda, MD}}
}

@phdthesis{mctavishthomass.InterpolatedAdaptiveContour2007,
  title = {Interpolated and Adaptive Contour {{3D}} Mesh Segmentation in Image Volumes},
  author = {{McTavish, Thomas S.}},
  year = {2007},
  langid = {english},
  school = {University of Colorado Denver},
  file = {/Users/tom/Zotero/storage/ABAKL5NR/00001.html}
}

@article{meekPalisadePatternMormyrid1991,
  title = {Palisade Pattern of Mormyrid {{Purkinje}} Cells: {{A}} Correlated Light and Electron Microscopic Study},
  shorttitle = {Palisade Pattern of Mormyrid {{Purkinje}} Cells},
  author = {Meek, J. and Nieuwenhuys, R.},
  year = {1991},
  journal = {Journal of Comparative Neurology},
  volume = {306},
  number = {1},
  pages = {156--192},
  issn = {1096-9861},
  doi = {10.1002/cne.903060111},
  abstract = {The present study is devoted to a detailed analysis of the structural and synaptic organization of mormyrid Purkinje cells in order to evaluate the possible functional significance of their dendritic palisade pattern. For this purpose, the properties of Golgi-impregnated as well as unimpregnated Purkinje cells in lobe C1 and C3 of the cerebellum of Gnathonemus petersii were light and electron microscopically analyzed, quantified, reconstructed, and mutually compared. Special attention was paid to the degree of regularity of their dendritic trees, their relations with Bergmann glia, and the distribution and numerical properties of their synaptic connections with parallel fibers, stellate cells, ``climbing'' fibers, and Purkinje axonal boutons. The highest degree of palisade specialization was encountered in lobe C1, where Purkinje cells have on average 50 palisade dendrites with a very regular distribution in a sagittal plane. Their spine density decreases from superficial to deep (from 14 to 6 per {$\mu$}m dendritic length), a gradient correlated with a decreasing parallel fiber density but an increasing parallel fiber diameter. Each Purkinje cell makes on average 75,000 synaptic contacts with parallel fibers, some of which are rather coarse (0.45 {$\mu$}m), and provided with numerous short collaterals. Climbing fibers do not climb, since their synaptic contacts are restricted to the ganglionic layer (i.e., the layer of Purkinje and eurydendroid projection cells), where they make about 130 synaptic contacts per cell with 2 or 3 clusters of thorns on the proximal dendrites. These clusters contain also a type of ``shunting'' elements that make desmosome-like junctions with both the climbing fiber boutons and the necks of the thorns. The axons of Purkinje cells in lobe C1 make small terminal arborizations, with about 20 boutons, that may be substantially (up to 500 {$\mu$}m) displaced rostrally or caudally with respect to the soma. Purkinje axonal boutons were observed to make synaptic contacts with eurydendroid projection cells and with the proximal dendritic and somatic receptive surface of Purkinje cells, where about 15 randomly distributed boutons per neuron occur. The organization of Purkinje cells in lobe C3 differs markedly from that in lobe C1 and seems to be less regular and specialized, although the overall palisade pattern is even more regular than in lobe C1 because of the absence of large eurydendroid neurons. However, individual neurons have a less regular dendritic tree, there is no apical-basal gradient in spine density or parallel fiber density and diameter, and there are no ``shunting'' elements in the climbing fiber glomeruli. Purkinje axonal boutons are not substantially displaced and have more but smaller boutons (on average about 70), which are not only contacting eurydendroid and Purkinje cells (about 40 boutons per cell), but also deeply located stellate neurons. As discussed in this study, none of the parameters analyzed is specifically and indissolubly correlated with the dendritic palisade pattern, and its functional significance consequently cannot be explained on the basis of a specific synaptic connectivity pattern. We suggest that palisade dendrites have a similar functional significance as their spines and may be considered as super- or giant spines, subserving optimal tuning of mormyrid Purkinje cells for specific spatio-temporal patterns of parallel fiber activity. Comparison of different types of Purkinje cell organizations as encountered in vertebrates shows two extremes: on the one hand, the mammalian configuration, probably specialized for optimal interactions with climbing fibers, and, on the other hand, the mormyrid palisade pattern, probably specialized for optimal interactions with parallel fibers.},
  langid = {english},
  keywords = {Bergmann glia,cerebellum,climbing fibers,Golgi-EM,parallel fibers,spines,stellate cells,teleosts},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.903060111},
  file = {/Users/tom/Zotero/storage/9XJY7NC6/cne.html}
}

@article{melInformationProcessingDendritic1994,
  title = {Information {{Processing}} in {{Dendritic Trees}}},
  author = {Mel, Bartlett W.},
  year = {1994},
  month = nov,
  journal = {Neural Computation},
  volume = {6},
  number = {6},
  pages = {1031--1085},
  doi = {10.1162/neco.1994.6.6.1031},
  langid = {english},
  file = {/Users/tom/Zotero/storage/YU3WNG2I/neco.1994.6.6.html}
}

@article{miglioreFunctionalRolesDistributed2010,
  title = {Functional Roles of Distributed Synaptic Clusters in the Mitral\textendash Granule Cell Network of the Olfactory Bulb},
  author = {Migliore, Michele and Hines, Michael and McTavish, Thomas S. and Shepherd, Gordon M.},
  year = {2010},
  journal = {Frontiers in integrative neuroscience},
  volume = {4},
  pages = {122}
}

@incollection{miglioreOlfactoryComputationMitralGranule2015,
  title = {Olfactory {{Computation}} in {{Mitral-Granule Cell Circuits}}},
  booktitle = {Encyclopedia of {{Computational Neuroscience}}},
  author = {Migliore, Michele and McTavish, Tom},
  editor = {Jaeger, Dieter and Jung, Ranu},
  year = {2015},
  pages = {2139--2142},
  publisher = {{Springer New York}},
  abstract = {SynonymsVertebrate olfactory bulb computationDefinitionOlfactory computation in mitral-granule cell circuits refers those operations between mitral and granule cells that can directly modulate the mitral cell firing behavior. In vertebrates, they are carried out by the reciprocal synapses established between mitral cells' lateral dendrites and spines on granule cell dendrites (Shepherd 2004).Detailed DescriptionMitral-granule cell circuits are formed by the continuous and dynamic reconfiguration of their synaptic connections during the entire life of the organism. Although, to date, there is no direct experimental evidence for plasticity of these synaptic connections, their sparse and distributed nature is well established (Willhite et al. 2006) and widely acknowledged as being the main mechanism responsible for the lateral inhibition effects observed in the olfactory bulb (Yokoi et al. 1995). This entry focuses on the circuital aspects of the mitral-granule reciprocal synapses, rather ...},
  copyright = {\textcopyright 2015 Springer Science+Business Media New York},
  isbn = {978-1-4614-6674-1 978-1-4614-6675-8},
  langid = {english},
  keywords = {Computation by Abstract Devices,Neurobiology,Neurosciences}
}

@misc{NeuralCodesFiring,
  title = {Neural Codes: {{Firing}} Rates and beyond - {{Google Search}}},
  howpublished = {https://www.google.com/search?q=Neural+codes\%3A+Firing+rates+and+beyond\&rlz=1C5CHFA\_enUS720US720\&oq=Neural+codes\%3A+Firing+rates+and+beyond\&aqs=chrome..69i57.609j0j7\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/tom/Zotero/storage/2US5NZUU/search.html}
}

@misc{NeuralNetworksDeep,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  journal = {Coursera},
  abstract = {Offered by DeepLearning.AI. In the first course of the Deep Learning Specialization, you will study the foundational concept of neural ... Enroll for free.},
  howpublished = {https://www.coursera.org/learn/neural-networks-deep-learning},
  langid = {english},
  file = {/Users/tom/Zotero/storage/38I56HSV/neural-networks-deep-learning.html}
}

@misc{nwankpaActivationFunctionsComparison2018,
  title = {Activation {{Functions}}: {{Comparison}} of Trends in {{Practice}} and {{Research}} for {{Deep Learning}}},
  shorttitle = {Activation {{Functions}}},
  author = {Nwankpa, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
  year = {2018},
  month = nov,
  number = {arXiv:1811.03378},
  eprint = {1811.03378},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1811.03378},
  abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/tom/Zotero/storage/UD78SLTC/1811.html}
}

@article{olahFeatureVisualization2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  volume = {2},
  number = {11},
  pages = {e7},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  abstract = {How neural networks build up their understanding of images},
  langid = {english}
}

@misc{oxforduniversitypressInformation2021,
  title = {Information},
  author = {{Oxford University Press}},
  year = {2021},
  journal = {Lexico.com},
  publisher = {{Oxford University Press}},
  abstract = {English dictionary definition of INFORMATION along with additional meanings, example sentences, and different ways to say.},
  langid = {english},
  file = {/Users/tom/Zotero/storage/AKI9J536/information.html}
}

@article{pennyRelationshipAxonalDendritic1988,
  title = {Relationship of the Axonal and Dendritic Geometry of Spiny Projection Neurons to the Compartmental Organization of the Neostriatum},
  author = {Penny, G. R. and Wilson, C. J. and Kitai, S. T.},
  year = {1988},
  journal = {Journal of Comparative Neurology},
  volume = {269},
  number = {2},
  pages = {275--289},
  issn = {1096-9861},
  doi = {10.1002/cne.902690211},
  abstract = {Intracellular injection of HRP combined with immunocytochemistry for [Leu]enkephalin was used to demonstrate striatal spiny neuron dendritic and local axonal arborizations in the same section as enkephalin-rich patches (striosomes). Cobalt intensification of the first DAB reaction prior to the immunoperoxidase steps resulted in good contrast between the black reaction product in the intracellularly labeled cells and the brown staining for [Leu]enkephalin. Serial reconstructions of the labeled cells and nearby boundaries between the enkephalin-rich striosomes and enkephalin-poor matrix allowed the relationship between the arborizations of the labeled cells and these boundaries to be established. It was also possible to examine the relationship to compartmental boundaries of a second neuronal class consisting of large, pallidallike neurons whose somatodendritic morphology was outlined by immunoperoxidase-labeled terminals. We found that spiny projection neurons in both compartments have dendritic arbors and local axonal collaterals that are confined by compartmental boundaries. The termination or recurvature of dendrites at such boundaries suggests that the cellular basis of striatal compartmental organization is provided by this class of striatal neuron. On the other hand, large pallidumlike striatal neurons were found to have dendrites that extend across compartmental boundaries. These results support previous reports that striatal spiny projection neurons preserve the compartmental segregation of parallel striatal input-output systems, whereas other classes of striatal neurons may serve to provide limited integration between compartments.},
  langid = {english},
  keywords = {intracellular labeling,Leuenkephalin,spiny neuron,striatal compartmental organization,striatum},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902690211},
  file = {/Users/tom/Zotero/storage/763MYL7E/cne.html}
}

@article{piccininiNeuralComputationComputational2013,
  title = {Neural {{Computation}} and the {{Computational Theory}} of {{Cognition}}},
  author = {Piccinini, Gualtiero and Bahar, Sonya},
  year = {2013},
  journal = {Cognitive Science},
  volume = {37},
  number = {3},
  pages = {453--488},
  issn = {1551-6709},
  doi = {10.1111/cogs.12012},
  abstract = {We begin by distinguishing computationalism from a number of other theses that are sometimes conflated with it. We also distinguish between several important kinds of computation: computation in a generic sense, digital computation, and analog computation. Then, we defend a weak version of computationalism\textemdash neural processes are computations in the generic sense. After that, we reject on empirical grounds the common assimilation of neural computation to either analog or digital computation, concluding that neural computation is sui generis. Analog computation requires continuous signals; digital computation requires strings of digits. But current neuroscientific evidence indicates that typical neural signals, such as spike trains, are graded like continuous signals but are constituted by discrete functional elements (spikes); thus, typical neural signals are neither continuous signals nor strings of digits. It follows that neural computation is sui generis. Finally, we highlight three important consequences of a proper understanding of neural computation for the theory of cognition. First, understanding neural computation requires a specially designed mathematical theory (or theories) rather than the mathematical theories of analog or digital computation. Second, several popular views about neural computation turn out to be incorrect. Third, computational theories of cognition that rely on non-neural notions of computation ought to be replaced or reinterpreted in terms of neural computation.},
  langid = {english},
  keywords = {Analog computation,Computational theory of cognition,Digital computation,Neural computation,Spike trains},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12012},
  file = {/Users/tom/Zotero/storage/J56AWCEG/Piccinini and Bahar - 2013 - Neural Computation and the Computational Theory of.pdf;/Users/tom/Zotero/storage/4G8689H7/cogs.html}
}

@article{pinedaEvolutionActionPotential2010,
  title = {Evolution of the {{Action Potential}}},
  author = {Pineda, Ricardo and Ribera, Angie},
  year = {2010},
  month = jan,
  journal = {Evolution of Nervous Systems},
  volume = {1},
  pages = {211--238},
  issn = {9780123708786},
  doi = {10.1016/B0-12-370878-8/00119-1},
  abstract = {Rapid signaling in the nervous system relies upon generation of action potentials (APs). In order to fire APs, neurons differentiate electrically excitable membrane properties. Study of many neurons indicates that electrical excitability is one of the earliest differentiated properties expressed by embryonic neurons. Further, electrical excitability is dynamically regulated during a neuron's lifetime, both as part of normal development as well as in response to activity. Because many different types of ion channel are involved in AP generation, regulation can simultaneously target many different proteins. In addition, regulation can occur simultaneously at multiple levels (e.g., transcriptional, post-transcriptional, translational, post-translational). Here, we review the molecular bases for developmental regulation of APs.},
  file = {/Users/tom/Zotero/storage/95732QSA/Pineda and Ribera - 2010 - Evolution of the Action Potential.pdf}
}

@incollection{pirahanchiPhysiologySodiumPotassium2022a,
  title = {Physiology, {{Sodium Potassium Pump}}},
  booktitle = {{{StatPearls}}},
  author = {Pirahanchi, Yasaman and Jessu, Rishita and Aeddula, Narothama R.},
  year = {2022},
  publisher = {{StatPearls Publishing}},
  address = {{Treasure Island (FL)}},
  abstract = {The~Na+ K+ pump is an electrogenic~transmembrane ATPase first discovered in 1957 and situated~in the outer plasma membrane of the cells; on the cytosolic side.[1][2] The Na+ K+ ATPase pumps 3 Na+ out of the cell and 2K+ that into the cell, for every single ATP consumed. The plasma membrane is a lipid bilayer that arranged asymmetrically, containing cholesterol, phospholipids, glycolipids, sphingolipid, and proteins within the membrane.[3][4] The Na+K+-ATPase pump helps to maintain osmotic equilibrium and membrane potential in cells. The sodium and potassium move against the concentration gradients.~The Na+ K+-ATPase pump maintains the gradient of a higher concentration of sodium extracellularly and a higher level of potassium intracellularly. The sustained concentration gradient is crucial for physiological processes in many organs and has an ongoing role in stabilizing the resting membrane potential of the cell, regulating the cell volume, and cell signal transduction.[2] It plays a crucial role on other physiological processes, such as maintenance of filtering waste products in the nephrons (kidneys), sperm motility, and production of the neuronal action potential.[5]~Furthermore,~the physiologic consequences of inhibiting the Na+-K+ ATPase are useful and the target in many pharmacologic applications.~ Na, K-ATPase is a crucial scaffolding protein that can interact with signaling proteins such as protein kinase C (PKC) and phosphoinositide 3-kinase (PI3K).[6]},
  copyright = {Copyright \textcopyright{} 2022, StatPearls Publishing LLC.},
  langid = {english},
  lccn = {NBK537088},
  pmid = {30725773},
  file = {/Users/tom/Zotero/storage/F7SKVUVC/NBK537088.html}
}

@article{pivovarovNaPumpNeurotransmitter2018,
  title = {Na+/{{K}}+-Pump and Neurotransmitter Membrane Receptors},
  author = {Pivovarov, Arkady S. and Calahorro, Fernando and Walker, Robert J.},
  year = {2018},
  month = nov,
  journal = {Invertebrate Neuroscience},
  volume = {19},
  number = {1},
  pages = {1},
  issn = {1439-1104},
  doi = {10.1007/s10158-018-0221-7},
  abstract = {Na+/K+-pump is an electrogenic transmembrane ATPase located in the outer plasma membrane of cells. The Na+/K+-ATPase pumps 3 sodium ions out of cells while pumping 2 potassium ions into cells. Both cations move against their concentration gradients. This enzyme's electrogenic nature means that it has a chronic role in stabilizing the resting membrane potential of the cell, in regulating the cell volume and in the signal transduction of the cell. This review will mainly consider the role of the Na+/K+-pump in neurons, with an emphasis on its role in modulating neurotransmitter receptor. Most of the literature on the modulation of neurotransmitter receptors refers to the situation in the mammalian nervous system, but the position is likely to be similar in most, if not all, invertebrate nervous systems.},
  langid = {english},
  keywords = {Na+/K+-pump,Neurotransmitter membrane receptors,Ouabain},
  file = {/Users/tom/Zotero/storage/XXYY64WZ/Pivovarov et al. - 2018 - Na+K+-pump and neurotransmitter membrane receptor.pdf}
}

@article{pouille*IntraglomerularGapJunctions2017,
  title = {Intraglomerular Gap Junctions Enhance Interglomerular Synchrony in a Sparsely Connected Olfactory Bulb Network},
  author = {Pouille*, Frederic and McTavish*, Thomas S. and Hunter, Lawrence E. and Restrepo, Diego and Schoppa, Nathan E.},
  year = {2017},
  month = sep,
  journal = {The Journal of Physiology},
  volume = {595},
  number = {17},
  pages = {5965--5986},
  issn = {1469-7793},
  doi = {10.1113/JP274408},
  abstract = {KEY POINTS: Despite sparse connectivity, population-level interactions between mitral cells (MCs) and granule cells (GCs) can generate synchronized oscillations in the rodent olfactory bulb. Intraglomerular gap junctions between MCs at the same glomerulus can greatly enhance synchronized activity of MCs at different glomeruli. The facilitating effect of intraglomerular gap junctions on interglomerular synchrony is through triggering of mutually synchronizing interactions between MCs and GCs. Divergent connections between MCs and GCs make minimal direct contribution to synchronous activity. ABSTRACT: A dominant feature of the olfactory bulb response to odour is fast synchronized oscillations at beta (15-40~Hz) or gamma (40-90~Hz) frequencies, thought to be involved in integration of olfactory signals. Mechanistically, the bulb presents an interesting case study for understanding how beta/gamma oscillations arise. Fast oscillatory synchrony in the activity of output mitral cells (MCs) appears to result from interactions with GABAergic granule cells (GCs), yet the incidence of MC-GC connections is very low, around 4\%. Here, we combined computational and experimental approaches to examine how oscillatory synchrony can nevertheless arise, focusing mainly on activity between 'non-sister' MCs affiliated with different glomeruli (interglomerular synchrony). In a sparsely connected model of MCs and GCs, we found first that interglomerular synchrony was generally quite low, but could be increased by a factor of 4 by physiological levels of gap junctional coupling between sister MCs at the same glomerulus. This effect was due to enhanced mutually synchronizing interactions between MC and GC populations. The potent role of gap junctions was confirmed in patch-clamp recordings in bulb slices from wild-type and connexin 36-knockout (KO) mice. KO reduced both beta and gamma local field potential oscillations as well as synchrony of inhibitory signals in pairs of non-sister MCs. These effects were independent of potential KO actions on network excitation. Divergent synaptic connections did not contribute directly to the vast majority of synchronized signals. Thus, in a sparsely connected network, gap junctions between a small subset of cells can, through population effects, greatly amplify oscillatory synchrony amongst unconnected cells.},
  langid = {english},
  pmcid = {PMC5577541},
  pmid = {28640508},
  keywords = {gap junction,olfactory bulb,synchronization},
  file = {/Users/tom/Zotero/storage/B6GCHESC/Pouille et al. - 2017 - Intraglomerular gap junctions enhance interglomeru.pdf}
}

@article{raichleAppraisingBrainEnergy2002,
  title = {Appraising the Brain's Energy Budget},
  author = {Raichle, Marcus E. and Gusnard, Debra A.},
  year = {2002},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {99},
  number = {16},
  pages = {10237--10239},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.172399499},
  file = {/Users/tom/Zotero/storage/6EPKBN98/Raichle and Gusnard - 2002 - Appraising the brain's energy budget.pdf}
}

@book{ramonycajalHistologieSystemeNerveux1909,
  title = {Histologie Du Syst\`eme Nerveux de l'homme \& Des Vert\'ebr\'es},
  author = {{Ram{\'o}n y Cajal}, Santiago},
  year = {1909},
  edition = {Ed. fran\c{c}aise rev. \& mise \`a jour par l'auteur, tr. de l'espagnol par L. Azoulay.},
  pages = {1--1012},
  publisher = {{Maloine}},
  address = {{Paris}},
  doi = {10.5962/bhl.title.48637},
  file = {/Users/tom/Zotero/storage/7Q24I6FZ/Ramón y Cajal - 1909 - Histologie du système nerveux de l'homme & des ver.pdf;/Users/tom/Zotero/storage/WDR8HAVD/48637.html}
}

@inproceedings{rebelloChannelrhodopsinMiceUse2013,
  title = {Channelrhodopsin {{Mice}} Use {{Temporal Information Encoded}} in the {{Olfactory Bulb}} for {{Odor Sensation}}.},
  booktitle = {33rd {{Annual Meeting}} of the {{Association}} for {{Chemoreception Sciences}} ({{AChemS}})},
  author = {Rebello, Michelle R. and McTavish, Thomas S. and Willhite, David C. and Shepherd, Gordon M. and Verhagen, Justus V.},
  year = {2013},
  month = apr,
  address = {{Huntington Beach, CA}}
}

@article{rebelloPerceptionOdorsLinked2014,
  title = {Perception of {{Odors Linked}} to {{Precise Timing}} in the {{Olfactory System}}},
  author = {Rebello, Michelle R. and McTavish, Thomas S. and Willhite, David C. and Short, Shaina M. and Shepherd, Gordon M. and Verhagen, Justus V.},
  year = {2014},
  journal = {PLoS Biol},
  volume = {12},
  number = {12},
  pages = {e1002021}
}

@article{restrepoTopFlexibleReading2009,
  title = {From the Top down: Flexible Reading of a Fragmented Odor Map},
  shorttitle = {From the Top Down},
  author = {Restrepo, Diego and Doucette, Wilder and Whitesell, Jennifer D. and McTavish, Thomas S. and Salcedo, Ernesto},
  year = {2009},
  journal = {Trends in neurosciences},
  volume = {32},
  number = {10},
  pages = {525--531}
}

@article{sakaryaPostSynapticScaffoldOrigin2007,
  title = {A {{Post-Synaptic Scaffold}} at the {{Origin}} of the {{Animal Kingdom}}},
  author = {Sakarya, Onur and Armstrong, Kathryn A. and Adamska, Maja and Adamski, Marcin and Wang, I.-Fan and Tidor, Bruce and Degnan, Bernard M. and Oakley, Todd H. and Kosik, Kenneth S.},
  year = {2007},
  month = jun,
  journal = {PLOS ONE},
  volume = {2},
  number = {6},
  pages = {e506},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0000506},
  abstract = {BackgroundThe evolution of complex sub-cellular structures such as the synapse requires the assembly of multiple proteins, each conferring added functionality to the integrated structure. Tracking the early evolution of synapses has not been possible without genomic information from the earliest branching animals. As the closest extant relatives to the Eumetazoa, Porifera (sponges) represent a pivotal group for understanding the evolution of nervous systems, because sponges lack neurons with clearly recognizable synapses, in contrast to eumetazoan animals.Methodology/Principal FindingsWe show that the genome of the demosponge Amphimedon queenslandica possesses a nearly complete set of post-synaptic protein homologs whose conserved interaction motifs suggest assembly into a complex structure. In the critical synaptic scaffold gene, dlg, residues that make hydrogen bonds and van der Waals interactions with the PDZ ligand are 100\% conserved between sponge and human, as is the motif organization of the scaffolds. Expression in Amphimedon of multiple post-synaptic gene homologs in larval flask cells further supports the existence of an assembled structure. Among the few post-synaptic genes absent from Amphimedon, but present in Eumetazoa, are receptor genes including the entire ionotropic glutamate receptor family.Conclusions/SignificanceHighly conserved protein interaction motifs and co-expression in sponges of multiple proteins whose homologs interact in eumetazoan synapses indicate that a complex protein scaffold was present at the origin of animals, perhaps predating nervous systems. A relatively small number of crucial innovations to this pre-existing structure may represent the founding changes that led to a post-synaptic element.},
  langid = {english},
  keywords = {Evolutionary genetics,Invertebrate genomics,Larvae,Nervous system,Phylogenetic analysis,Plant genomics,Protein domains,Sponges},
  file = {/Users/tom/Zotero/storage/8D92CBV4/Sakarya et al. - 2007 - A Post-Synaptic Scaffold at the Origin of the Anim.pdf;/Users/tom/Zotero/storage/GPA25LSB/article.html}
}

@article{serenoCaudalTopographicNucleus1987,
  title = {Caudal Topographic Nucleus Isthmi and the Rostral Nontopographic Nucleus Isthmi in the Turtle, Pseudemys Scripta},
  author = {Sereno, Martin I. and Ulinski, Philip S.},
  year = {1987},
  journal = {Journal of Comparative Neurology},
  volume = {261},
  number = {3},
  pages = {319--346},
  issn = {1096-9861},
  doi = {10.1002/cne.902610302},
  abstract = {Isthmotectal projections in turtles were examined by making serial section reconstructions of axonal and dendritic arborizations that were anterogradely or retrogradely filled with HRP. Two prominent tectal-recipient isthmic nuclei\textendash the caudal magnocellular nucleus isthmi (Imc) and the rostral magnocellular nucleus isthmi (Imr)\textendash exhibited strikingly different patterns of organization. Imc cells have flattened, bipolar dendritic fields that cover a few percent of the area of the cell plate constituting the nucleus and they project topographically to the ipsilateral tectum without local axon branches. The topography was examined explicitly at the single-cell level by using cases with two injections at widely separated tectal loci. Each Imc axon terminates as a compact swarm of several thousand boutons placed mainly in the upper central gray and superficial gray layers. One Imc terminal spans less that 1\% of the tectal surface. Imr cells, by contrast, have large, sparsely branched dendritic fields overlapped by local axon collaterals while distally, their axons nontopographically innervate not only the deeper layers of the ipsilateral tectum but also ipsilateral Imc. Imr receives a nontopographic tectal input that contrasts with the topographic tectal input to Imc. Previous work on nucleus isthmi emphasized the role of the contralateral isthmotectal projection (which originates from a third isthmic nucleus in turtles) in mediating binocular interactions in the tectum. The present results on the two different but overlapping ipsilateral tecto-isthmo-tectal circuits set up by Imc and Imr are discussed in the light of physiological evidence for selective attention effects and local-global interactions in the tectum.},
  langid = {english},
  keywords = {models of attention,parabigeminal nucleus,stimulus-specific surrounds,superior colliculus,tectum},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902610302},
  file = {/Users/tom/Zotero/storage/KMKFEAZT/cne.html}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {1948},
  month = jul,
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  file = {/Users/tom/Zotero/storage/2H2MICKX/Shannon - 1948 - A mathematical theory of communication.pdf;/Users/tom/Zotero/storage/VBFM5GJY/6773024.html}
}

@inproceedings{shortEffectGlomerularInput2014,
  title = {The Effect of Glomerular Input Patterns on Mitral Cell Responses in the Olfactory Bulb},
  booktitle = {34th {{Annual Meeting}} of the {{Association}} for {{Chemoreception Sciences}} ({{AChemS}})},
  author = {Short, Shaina M. and McTavish, Thomas S. and Morse, Thomas M. and Shepherd, Gordon M. and Verhagen, Justus V.},
  year = {2014},
  month = apr,
  address = {{Bonita Springs, FL}}
}

@article{shortRespirationGatesSensory2016,
  title = {Respiration {{Gates Sensory Input Responses}} in the {{Mitral Cell Layer}} of the {{Olfactory Bulb}}},
  author = {Short, Shaina M. and Morse, Thomas M. and McTavish, Thomas S. and Shepherd, Gordon M. and Verhagen, Justus V.},
  year = {2016},
  month = dec,
  journal = {PLOS ONE},
  volume = {11},
  number = {12},
  pages = {e0168356},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0168356},
  abstract = {Respiration plays an essential role in odor processing. Even in the absence of odors, oscillating excitatory and inhibitory activity in the olfactory bulb synchronizes with respiration, commonly resulting in a burst of action potentials in mammalian mitral/tufted cells (MTCs) during the transition from inhalation to exhalation. This excitation is followed by inhibition that quiets MTC activity in both the glomerular and granule cell layers. Odor processing is hypothesized to be modulated by and may even rely on respiration-mediated activity, yet exactly how respiration influences sensory processing by MTCs is still not well understood. By using optogenetics to stimulate discrete sensory inputs in vivo, it was possible to temporally vary the stimulus to occur at unique phases of each respiration. Single unit recordings obtained from the mitral cell layer were used to map spatiotemporal patterns of glomerular evoked responses that were unique to stimulations occurring during periods of inhalation or exhalation. Sensory evoked activity in MTCs was gated to periods outside phasic respiratory mediated firing, causing net shifts in MTC activity across the cycle. In contrast, odor evoked inhibitory responses appear to be permitted throughout the respiratory cycle. Computational models were used to further explore mechanisms of inhibition that can be activated by respiratory activity and influence MTC responses. In silico results indicate that both periglomerular and granule cell inhibition can be activated by respiration to internally gate sensory responses in the olfactory bulb. Both the respiration rate and strength of lateral connectivity influenced inhibitory mechanisms that gate sensory evoked responses.},
  keywords = {Action potentials,Cell polarity,Granule cells,Inhalation,Neurons,Olfactory bulb,Respiratory burst,Synapses},
  file = {/Users/tom/Zotero/storage/QE2D6M8U/Short et al. - 2016 - Respiration Gates Sensory Input Responses in the M.pdf;/Users/tom/Zotero/storage/T5VTP5FC/article.html}
}

@inproceedings{shortSpatiotemporalInputoutputFunction2015,
  title = {The Spatiotemporal Input-Output Function of the Olfactory Bulb Is Modulated by Respiratory Cycle Activity},
  booktitle = {35th {{Annual Meeting}} of the {{Association}} for {{Chemoreception Sciences}} ({{AChemS}})},
  author = {Short, Shaina M. and McTavish, Thomas S. and Morse, Thomas M. and Shepherd, Gordon M. and Verhagen, Justus V.},
  year = {2015},
  month = apr,
  address = {{Bonita Springs, FL}}
}

@book{silverSignalNoiseWhy2015,
  title = {The {{Signal}} and the {{Noise}}: {{Why So Many Predictions Fail--but Some Don}}'t},
  shorttitle = {The {{Signal}} and the {{Noise}}},
  author = {Silver, Nate},
  year = {2015},
  month = feb,
  edition = {Illustrated edition},
  publisher = {{Penguin Books}},
  address = {{New York, NY}},
  isbn = {978-0-14-312508-2},
  langid = {english}
}

@article{stanleyReadingWritingNeural2013,
  title = {Reading and Writing the Neural Code},
  author = {Stanley, Garrett B.},
  year = {2013},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {16},
  number = {3},
  pages = {259--263},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3330},
  abstract = {In this Perspective, the author examines how reading and writing the neural code may be linked. He reviews evidence defining the nature of neural coding of sensory input and asks how these constraints, particularly precise timing, might be critical for approaches that seek to `write the neural code' through the artificial control of microcircuits to activate downstream structures.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computational neuroscience;Sensorimotor processing Subject\_term\_id: computational-neuroscience;sensorimotor-processing},
  file = {/Users/tom/Zotero/storage/9CEID4AA/nn.html}
}

@book{stevensonOxfordDictionaryEnglish2010,
  title = {Oxford {{Dictionary}} of {{English}}},
  author = {Stevenson, Angus},
  year = {2010},
  month = aug,
  publisher = {{OUP Oxford}},
  abstract = {The foremost single volume authority on the English language, the Oxford Dictionary of English is at the forefront of language research, focusing on English as it is used today. It is informed by the most up-to-date evidence from the largest language research programme in the world, including the two-billion-word Oxford English Corpus. This new edition includes thousands of brand-new words and senses, as well as up-to-date encyclopedic information, and extensive appendices covering topics such as countries, heads of state, and chemical elements. New features include Word Trends which showcase language research based on the Oxford English Corpus and illuminate the extraordinary stories behind fast-changing words of everyday English. 12 months' access to Oxford's premium online dictionary and thesaurus service is included with this book, so you can get accurate definitions and synonyms wherever you are. Find out more about our living language using Oxford Dictionaries Pro - updated regularly with the latest changes to words and meanings, so you have the most accurate picture of English available. Use the thousands of audio pronunciations to hear how words are spoken. Improve your confidence in writing with helpful grammar and punctuation guides, full thesaurus information, style and usage help, and much more.},
  googlebooks = {anecAQAAQBAJ},
  isbn = {978-0-19-957112-3},
  langid = {english},
  keywords = {Foreign Language Study / English as a Second Language,Language Arts \& Disciplines / Journalism,Reference / Dictionaries}
}

@misc{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  month = jun,
  number = {arXiv:1906.02243},
  eprint = {1906.02243},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/tom/Zotero/storage/2VGTQ66D/Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf;/Users/tom/Zotero/storage/YK8B6RUV/1906.html}
}

@book{stuartDendrites2008,
  title = {Dendrites},
  author = {Stuart, Greg and Spruston, Nelson and H{\"a}usser, Michael},
  year = {2008},
  edition = {Second},
  publisher = {{Oxford University Press}},
  address = {{Oxford; New York}},
  isbn = {978-0-19-856656-4},
  langid = {english},
  annotation = {OCLC: 273081399}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7298594},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  keywords = {Computer architecture,Computer vision,Convolutional codes,Neural networks,Object detection,Sparse matrices,Visualization},
  file = {/Users/tom/Zotero/storage/GBQ4QEYF/Szegedy et al. - 2015 - Going deeper with convolutions.pdf;/Users/tom/Zotero/storage/BD89BEDI/7298594.html}
}

@article{terashimaObservationsCerebellumNormalreeler1986,
  title = {Observations on the Cerebellum of Normal-Reeler Mutant Mouse Chimera},
  author = {Terashima, Toshio and Inoue, Kaoru and Inoue, Yoshiro and Yokoyama, Minesuke and Mikoshiba, Katsuhiko},
  year = {1986},
  journal = {Journal of Comparative Neurology},
  volume = {252},
  number = {2},
  pages = {264--278},
  issn = {1096-9861},
  doi = {10.1002/cne.902520209},
  abstract = {The normal-reeler chimera mouse (+/+ {$\leftrightarrow$} rl/rl) provides an experimental system in which an analysis of the migration of immature neurons in the cerebellum can be accomplished. In the present study, five chimera mice were produced from embryos of the wild-type control (C57Bl/6N) and the reeler mutant mouse (BALB/c) by the aggregation technique. The isozyme pattern of glucosephosphate isomerase (GPI) revealed that the brain tissue in the chimera contained both isozymes of the BALB/c (reeler) and C57Bl/6N (normal) strains, implying that internal mosaicism of the cerebellum truly existed. We found no abnormality in the cerebellum of the chimera mouse: the neuronal and glial subpopulations revealed no difference from those of the control. Such normalization of the cerebellum in the chimera suggests that the disturbance of neuronal migration in the reeler is attributable to an abnormal cell-to-cell interaction between migrating young neurons and the radial glial cells.},
  langid = {english},
  keywords = {Golgi epithelial cell,granule cell,neuronal migration,Purkinje cell},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902520209},
  file = {/Users/tom/Zotero/storage/U8S5Y8DF/cne.html}
}

@article{treueNeuralCorrelatesAttention2001,
  title = {Neural Correlates of Attention in Primate Visual Cortex},
  author = {Treue, Stefan},
  year = {2001},
  month = may,
  journal = {Trends in Neurosciences},
  volume = {24},
  number = {5},
  pages = {295--300},
  issn = {0166-2236},
  doi = {10.1016/S0166-2236(00)01814-2},
  abstract = {The processing of visual information combines bottom-up sensory aspects with top-down influences, most notably attentional processes. Attentional influences have now been demonstrated throughout visual cortex, and their influence on the processing of visual information is profound. Neuronal responses to attended locations or stimulus features are enhanced, whereas those from unattended locations or features are suppressed. This influence of attention increases as one ascends the hierarchy of visual areas in primate cortex, ultimately resulting in a neural representation of the visual world that is dominated by the behavioral relevance of the information, rather than designed to provide an accurate and complete description of it. This realization has led to a rethinking of the role of areas that have previously been considered to be `purely sensory'.},
  langid = {english},
  keywords = {attention,extrastriate cortex,neurophysiology,vision},
  file = {/Users/tom/Zotero/storage/KH59IAD7/Treue - 2001 - Neural correlates of attention in primate visual c.pdf;/Users/tom/Zotero/storage/SMV3RD2W/S0166223600018142.html}
}

@article{trueGeneCooptionPhysiological2002,
  title = {Gene Co-Option in Physiological and Morphological Evolution},
  author = {True, John R. and Carroll, Sean B.},
  year = {2002},
  journal = {Annual Review of Cell and Developmental Biology},
  volume = {18},
  pages = {53--80},
  issn = {1081-0706},
  doi = {10.1146/annurev.cellbio.18.020402.140619},
  abstract = {Co-option occurs when natural selection finds new uses for existing traits, including genes, organs, and other body structures. Genes can be co-opted to generate developmental and physiological novelties by changing their patterns of regulation, by changing the functions of the proteins they encode, or both. This often involves gene duplication followed by specialization of the resulting paralogous genes into particular functions. A major role for gene co-option in the evolution of development has long been assumed, and many recent comparative developmental and genomic studies have lent support to this idea. Although there is relatively less known about the molecular basis of co-option events involving developmental pathways, much can be drawn from well-studied examples of the co-option of structural proteins. Here, we summarize several case studies of both structural gene and developmental genetic circuit co-option and discuss how co-option may underlie major episodes of adaptive change in multicellular organisms. We also examine the phenomenon of intraspecific variability in gene expression patterns, which we propose to be one form of material for the co-option process. We integrate this information with recent models of gene family evolution to provide a framework for understanding the origin of co-optive evolution and the mechanisms by which natural selection promotes evolutionary novelty by inventing new uses for the genetic toolkit.},
  langid = {english},
  pmid = {12142278},
  keywords = {Adaptation; Physiological,Animals,Body Patterning,Evolution; Molecular,Gene Duplication,Gene Expression Regulation; Developmental,Genes,Genetic Variation,Humans}
}

@misc{velickovicGraphAttentionNetworks2018a,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/tom/Zotero/storage/SSE3XT9Q/Veličković et al. - 2018 - Graph Attention Networks.pdf;/Users/tom/Zotero/storage/JFJQCJ5T/1710.html}
}

@article{wangNeuralEnergySupplyConsumption2017,
  title = {Neural {{Energy Supply-Consumption Properties Based}} on {{Hodgkin-Huxley Model}}},
  author = {Wang, Yihong and Wang, Rubin and Xu, Xuying},
  year = {2017},
  month = feb,
  journal = {Neural Plasticity},
  volume = {2017},
  pages = {e6207141},
  publisher = {{Hindawi}},
  issn = {2090-5904},
  doi = {10.1155/2017/6207141},
  abstract = {Electrical activity is the foundation of the neural system. Coding theories that describe neural electrical activity by the roles of action potential timing or frequency have been thoroughly studied. However, an alternative method to study coding questions is the energy method, which is more global and economical. In this study, we clearly defined and calculated neural energy supply and consumption based on the Hodgkin-Huxley model, during firing action potentials and subthreshold activities using ion-counting and power-integral model. Furthermore, we analyzed energy properties of each ion channel and found that, under the two circumstances, power synchronization of ion channels and energy utilization ratio have significant differences. This is particularly true of the energy utilization ratio, which can rise to above 100\% during subthreshold activity, revealing an overdraft property of energy use. These findings demonstrate the distinct status of the energy properties during neuronal firings and subthreshold activities. Meanwhile, after introducing a synapse energy model, this research can be generalized to energy calculation of a neural network. This is potentially important for understanding the relationship between dynamical network activities and cognitive behaviors.},
  langid = {english},
  file = {/Users/tom/Zotero/storage/CXPU8EC5/Wang et al. - 2017 - Neural Energy Supply-Consumption Properties Based .pdf;/Users/tom/Zotero/storage/X4355JJJ/6207141.html}
}

@techreport{warkentienChartingProgressHewlett2017,
  title = {Charting the {{Progress}} of the {{Hewlett Foundation}}'s {{Deeper Learning Strategy}}},
  author = {Warkentien, Siri and Charles, Karen and Knapp, Laura and Silver, David},
  year = {2017},
  month = feb,
  institution = {{RTI International}},
  langid = {english},
  file = {/Users/tom/Zotero/storage/5M6NRQ8M/Warkentien et al. - Charting the Progress of the Hewlett Foundation’s .pdf}
}

@misc{wikipediacontributorsIntelligenceWikipediaFree2021,
  title = {Intelligence --- {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia contributors}},
  year = {2021},
  month = jun,
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1026368502},
  file = {/Users/tom/Zotero/storage/6IW8SCKQ/index.html}
}

@article{xiaGraphLearningSurvey2021,
  title = {Graph {{Learning}}: {{A Survey}}},
  shorttitle = {Graph {{Learning}}},
  author = {Xia, Feng and Sun, Ke and Yu, Shuo and Aziz, Abdul and Wan, Liangtian and Pan, Shirui and Liu, Huan},
  year = {2021},
  month = apr,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {2},
  number = {2},
  eprint = {2105.00696},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {109--127},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3076021},
  abstract = {Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.},
  archiveprefix = {arXiv},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,I.2.6},
  file = {/Users/tom/Zotero/storage/W3ZK7YQI/Xia et al. - 2021 - Graph Learning A Survey.pdf;/Users/tom/Zotero/storage/FFNQP8CM/2105.html}
}

@article{yangNeuropeptideLikeImmunoreactiveCells1986,
  title = {Neuropeptide-{{Like}} Immunoreactive Cells in the Retina of the Larval Tiger Salamander: {{Attention}} to the Symmetry of Dendritic Projections},
  shorttitle = {Neuropeptide-{{Like}} Immunoreactive Cells in the Retina of the Larval Tiger Salamander},
  author = {Yang, Chen-Yu and Yazulla, Stephen},
  year = {1986},
  journal = {Journal of Comparative Neurology},
  volume = {248},
  number = {1},
  pages = {105--118},
  issn = {1096-9861},
  doi = {10.1002/cne.902480108},
  abstract = {Light microscopic immunocytochemistry was used to study the morphology of cells that showed immunoreactivity (IR) to antisera against substance P (SP), glucagon (GLU), met enkephalin (ENK), and somatostatin (SS) in the retina of the larval tiger salamander. Both vertical sections and retinal whole mounts were studied. All four antisera labeled amacrine cells in the inner nuclear layer and cells in the ganglion cell layer (GCL). GLUIR cells had processes stratified throughout the inner plexiform layer (IPL), whereas the other three types had bistratified projections in laminae 1 and 5, withlamina 5 being broader and more dense. Two types of SS-IR amacrine cell were observed. As seen in retinal whole mount, most GLU-IR and ENKIR amacrine cells had processes that were symmetrically distributed about the soma, whereas processes of SS- and SP-IR amacrine cells were markedly asymmetrical. The dendritic fields of SP-IR amacrine cells were selectively oriented toward the periphery of the retina in the nasal, temporal, and dorsal areas. Immunoreactive cells in the GCL had fine projections into the IPL and in addition gave rise to two large oriented processes proximal to the soma that projected in opposite directions for 100\textendash 200 {$\mu$}m. The oriented processes often showed further branching; they appeared to be along the radiated lines from the optic disc but did not enter it. The implications of the selectively oriented processes of SP-IR amacrine cells and cells in the GCL are discussed.},
  langid = {english},
  keywords = {enkephalin,glucagon,immunocytochemistry,somatostatin,substance P},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.902480108},
  file = {/Users/tom/Zotero/storage/5ERGMX6A/cne.html}
}

@inproceedings{yuEmergentSpatiallyDistributed2012,
  title = {Emergent Spatially Distributed Synaptic Clusters in a Large-Scale Network Model of the Olfactory Bulb},
  booktitle = {32nd {{Annual Meeting}} of the {{Association}} for {{Chemoreception Sciences}} ({{AChemS}})},
  author = {Yu, Yuguo and McTavish, Thomas S. and Hines, Michael L. and Shepherd, Gordon M. and Migliore, Michele},
  year = {2012},
  month = apr,
  address = {{Huntington Beach, CA}}
}

@article{yuSparseDistributedRepresentation2013,
  title = {Sparse Distributed Representation of Odors in a Large-Scale Olfactory Bulb Circuit},
  author = {Yu, Yuguo and McTavish, Thomas S. and Hines, Michael L. and Shepherd, Gordon M. and Valenti, Cesare and Migliore, Michele},
  year = {2013},
  journal = {PLoS Comput Biol},
  volume = {9},
  number = {3},
  pages = {e1003014}
}

@article{zeldenrustNeuralCodingBursts2018,
  title = {Neural {{Coding With Bursts}}\textemdash{{Current State}} and {{Future Perspectives}}},
  author = {Zeldenrust, Fleur and Wadman, Wytse J. and Englitz, Bernhard},
  year = {2018},
  journal = {Frontiers in Computational Neuroscience},
  volume = {12},
  pages = {48},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00048},
  abstract = {Neuronal action potentials or spikes provide a long-range, noise-resistant means of communication between neurons. As point processes single spikes contain little information in themselves, i.e., outside the context of spikes from other neurons. Moreover, they may fail to cross a synapse. A burst, which consists of a short, high frequency train of spikes, will more reliably cross a synapse, increasing the likelihood of eliciting a postsynaptic spike, depending on the specific short-term plasticity at that synapse. Both the number and the temporal pattern of spikes in a burst provide a coding space that lies within the temporal integration realm of single neurons. Bursts have been observed in many species, including the non-mammalian, and in brain regions that range from subcortical to cortical. Despite their widespread presence and potential relevance, the uncertainties of how to classify bursts seems to have limited the research into the coding possibilities for bursts. The present series of research articles provides new insights into the relevance and interpretation of bursts across different neural circuits, and new methods for their analysis. Here, we provide a succinct introduction to the history of burst coding and an overview of recent work on this topic.},
  file = {/Users/tom/Zotero/storage/YE4B6LSX/Zeldenrust et al. - 2018 - Neural Coding With Bursts—Current State and Future.pdf}
}


